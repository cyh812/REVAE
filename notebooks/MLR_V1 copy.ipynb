{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a9d82bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import json\n",
    "import torch\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c8eff85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_colors: 8 ['blue', 'brown', 'cyan', 'gray', 'green', 'purple', 'red', 'yellow']\n",
      "num_shapes: 3 ['cube', 'cylinder', 'sphere']\n",
      "max_objects: 10\n"
     ]
    }
   ],
   "source": [
    "# 构建数据集\n",
    "from torch.utils.data import DataLoader,Subset\n",
    "from src.dataset import build_global_vocab_and_maxcount, CLEVRMultiLabelByImage\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "clevr_root = \"../CLEVR_v1.0\"\n",
    "\n",
    "# 全局统计：colors/shapes/max_objects（train+val+test）\n",
    "colors, shapes, max_objects, _ = build_global_vocab_and_maxcount(clevr_root, splits=(\"train\",\"val\"))\n",
    "\n",
    "print(\"num_colors:\", len(colors), colors)\n",
    "print(\"num_shapes:\", len(shapes), shapes)\n",
    "print(\"max_objects:\", max_objects)\n",
    "\n",
    "tfm64 = T.Compose([\n",
    "    T.Resize((64, 64)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "tfm128 = T.Compose([\n",
    "    T.Resize((128, 128)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "train_ds_64 = CLEVRMultiLabelByImage(\n",
    "    clevr_root=clevr_root,\n",
    "    split=\"train\",\n",
    "    colors=colors,\n",
    "    shapes=shapes,\n",
    "    max_objects=max_objects,\n",
    "    transform=tfm64\n",
    ")\n",
    "\n",
    "train_ds_128 = CLEVRMultiLabelByImage(\n",
    "    clevr_root=clevr_root,\n",
    "    split=\"train\",\n",
    "    colors=colors,\n",
    "    shapes=shapes,\n",
    "    max_objects=max_objects,\n",
    "    transform=tfm128\n",
    ")\n",
    "\n",
    "train_dl_64 = DataLoader(train_ds_64, batch_size=100, shuffle=True, num_workers=0)\n",
    "train_dl_128 = DataLoader(train_ds_128, batch_size=100, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77d5295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "from src.model import REVAE_V64, REVAE_V128\n",
    "\n",
    "revae_64 = REVAE_V64()\n",
    "revae_128 = REVAE_V128()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddafba03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\github file\\REVAE\\src\\train.py:137: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(cfg.use_amp and device.type == \"cuda\"))\n",
      "d:\\github file\\REVAE\\src\\train.py:152: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(cfg.use_amp and device.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] epoch 1 step 0/700 total=9230.7988 recon=9230.4375 kl=0.3617 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 1 step 100/700 total=8371.2178 recon=8361.0195 kl=10.1978 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 1 step 200/700 total=8347.8906 recon=8336.1846 kl=11.7064 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 1 step 300/700 total=8346.2080 recon=8335.2510 kl=10.9572 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 1 step 400/700 total=8350.2139 recon=8338.2588 kl=11.9550 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 1 step 500/700 total=8344.2939 recon=8332.3945 kl=11.8995 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 1 step 600/700 total=8348.4209 recon=8336.2822 kl=12.1390 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch2\n",
      "[train] epoch 2 step 0/700 total=8345.2646 recon=8333.5488 kl=11.7155 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 2 step 100/700 total=8351.3525 recon=8339.6963 kl=11.6567 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 2 step 200/700 total=8344.4463 recon=8333.0176 kl=11.4283 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 2 step 300/700 total=8352.8643 recon=8341.1006 kl=11.7637 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 2 step 400/700 total=8346.9199 recon=8335.2773 kl=11.6429 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 2 step 500/700 total=8338.9648 recon=8326.4629 kl=12.5015 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 2 step 600/700 total=8350.2354 recon=8338.5889 kl=11.6467 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch3\n",
      "[train] epoch 3 step 0/700 total=8346.0176 recon=8334.0244 kl=11.9929 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 3 step 100/700 total=8350.6045 recon=8338.5957 kl=12.0087 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 3 step 200/700 total=8349.1367 recon=8337.3789 kl=11.7574 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 3 step 300/700 total=8353.0127 recon=8341.0400 kl=11.9730 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 3 step 400/700 total=8331.1240 recon=8318.9121 kl=12.2115 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 3 step 500/700 total=8343.0176 recon=8330.8887 kl=12.1292 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 3 step 600/700 total=8336.0098 recon=8324.3613 kl=11.6484 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch4\n",
      "[train] epoch 4 step 0/700 total=8336.6494 recon=8323.6357 kl=13.0137 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 4 step 100/700 total=8333.6729 recon=8319.9902 kl=13.6829 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 4 step 200/700 total=8348.6553 recon=8336.1172 kl=12.5385 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 4 step 300/700 total=8329.6240 recon=8316.0479 kl=13.5758 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 4 step 400/700 total=8346.4756 recon=8333.8486 kl=12.6270 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 4 step 500/700 total=8352.6904 recon=8339.9707 kl=12.7194 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 4 step 600/700 total=8338.0811 recon=8325.9834 kl=12.0977 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch5\n",
      "[train] epoch 5 step 0/700 total=8336.8584 recon=8323.7539 kl=13.1047 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 5 step 100/700 total=8339.7598 recon=8326.8965 kl=12.8631 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 5 step 200/700 total=8343.9102 recon=8331.3896 kl=12.5208 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 5 step 300/700 total=8334.5684 recon=8321.8145 kl=12.7540 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 5 step 400/700 total=8341.7158 recon=8328.6494 kl=13.0660 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 5 step 500/700 total=8328.6318 recon=8316.1533 kl=12.4786 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 5 step 600/700 total=8336.7529 recon=8323.9463 kl=12.8066 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch6\n",
      "[train] epoch 6 step 0/700 total=8351.6533 recon=8339.7520 kl=11.9012 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 6 step 100/700 total=8339.8369 recon=8325.8965 kl=13.9404 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 6 step 200/700 total=8331.0713 recon=8317.9033 kl=13.1676 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 6 step 300/700 total=8347.6729 recon=8334.3398 kl=13.3326 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 6 step 400/700 total=8337.4639 recon=8325.3867 kl=12.0773 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 6 step 500/700 total=8347.7695 recon=8333.8711 kl=13.8989 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 6 step 600/700 total=8343.9189 recon=8330.4951 kl=13.4234 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch7\n",
      "[train] epoch 7 step 0/700 total=8352.0703 recon=8338.9395 kl=13.1311 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 7 step 100/700 total=8340.6572 recon=8327.6982 kl=12.9590 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 7 step 200/700 total=8345.3691 recon=8332.0127 kl=13.3563 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 7 step 300/700 total=8346.6963 recon=8333.5850 kl=13.1117 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 7 step 400/700 total=8338.2324 recon=8325.2246 kl=13.0079 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 7 step 500/700 total=8339.9951 recon=8327.2441 kl=12.7505 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 7 step 600/700 total=8338.0957 recon=8325.3438 kl=12.7515 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch8\n",
      "[train] epoch 8 step 0/700 total=8341.9229 recon=8329.6025 kl=12.3205 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 8 step 100/700 total=8345.1406 recon=8333.0215 kl=12.1194 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 8 step 200/700 total=8334.1396 recon=8321.9521 kl=12.1874 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 8 step 300/700 total=8348.9971 recon=8336.7432 kl=12.2538 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 8 step 400/700 total=8338.2227 recon=8324.6484 kl=13.5743 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 8 step 500/700 total=8331.3262 recon=8318.6113 kl=12.7147 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 8 step 600/700 total=8333.3223 recon=8320.7764 kl=12.5460 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch9\n",
      "[train] epoch 9 step 0/700 total=8327.0879 recon=8313.0928 kl=13.9953 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 9 step 100/700 total=8336.4424 recon=8323.8838 kl=12.5582 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 9 step 200/700 total=8320.1689 recon=8305.4502 kl=14.7192 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 9 step 300/700 total=8338.1172 recon=8325.2715 kl=12.8458 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 9 step 400/700 total=8336.2959 recon=8323.4033 kl=12.8921 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 9 step 500/700 total=8337.0469 recon=8323.4062 kl=13.6402 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 9 step 600/700 total=8336.4326 recon=8322.7363 kl=13.6965 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch10\n",
      "[train] epoch 10 step 0/700 total=8334.8135 recon=8321.8340 kl=12.9791 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 10 step 100/700 total=8334.6904 recon=8321.1328 kl=13.5575 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 10 step 200/700 total=8339.5684 recon=8326.9834 kl=12.5850 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 10 step 300/700 total=8345.4043 recon=8331.8906 kl=13.5136 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 10 step 400/700 total=8341.0430 recon=8328.2119 kl=12.8308 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 10 step 500/700 total=8338.3164 recon=8325.0771 kl=13.2395 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 10 step 600/700 total=8339.5400 recon=8324.1699 kl=15.3699 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch11\n",
      "[train] epoch 11 step 0/700 total=8339.5928 recon=8326.5264 kl=13.0664 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 11 step 100/700 total=8337.4385 recon=8322.8613 kl=14.5770 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 11 step 200/700 total=8327.7588 recon=8313.1104 kl=14.6485 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 11 step 300/700 total=8332.7754 recon=8319.0430 kl=13.7327 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 11 step 400/700 total=8342.2812 recon=8328.6875 kl=13.5936 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 11 step 500/700 total=8341.1494 recon=8328.2197 kl=12.9294 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 11 step 600/700 total=8329.7607 recon=8315.2021 kl=14.5590 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch12\n",
      "[train] epoch 12 step 0/700 total=8340.1748 recon=8326.4512 kl=13.7239 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 12 step 100/700 total=8315.4326 recon=8301.0830 kl=14.3495 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 12 step 200/700 total=8328.8174 recon=8315.6006 kl=13.2166 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 12 step 300/700 total=8345.2881 recon=8331.4570 kl=13.8313 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 12 step 400/700 total=8326.8906 recon=8312.5000 kl=14.3904 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 12 step 500/700 total=8335.6309 recon=8321.6553 kl=13.9759 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 12 step 600/700 total=8329.6445 recon=8315.9834 kl=13.6612 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch13\n",
      "[train] epoch 13 step 0/700 total=8337.9014 recon=8324.7246 kl=13.1770 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 13 step 100/700 total=8338.6729 recon=8324.1387 kl=14.5342 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 13 step 200/700 total=8350.4844 recon=8336.5156 kl=13.9683 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 13 step 300/700 total=8354.1572 recon=8339.8994 kl=14.2577 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 13 step 400/700 total=8337.4580 recon=8323.3887 kl=14.0695 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 13 step 500/700 total=8342.9121 recon=8329.3369 kl=13.5755 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 13 step 600/700 total=8355.2383 recon=8340.9443 kl=14.2935 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch14\n",
      "[train] epoch 14 step 0/700 total=8338.2090 recon=8324.3203 kl=13.8891 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 14 step 100/700 total=8332.5811 recon=8319.5010 kl=13.0802 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 14 step 200/700 total=8326.5361 recon=8313.0762 kl=13.4597 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 14 step 300/700 total=8326.9434 recon=8312.0020 kl=14.9416 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 14 step 400/700 total=8348.7236 recon=8335.1260 kl=13.5978 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 14 step 500/700 total=8343.5469 recon=8329.6924 kl=13.8550 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 14 step 600/700 total=8346.0840 recon=8332.2227 kl=13.8617 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch15\n",
      "[train] epoch 15 step 0/700 total=8343.0664 recon=8328.1162 kl=14.9498 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 15 step 100/700 total=8340.4619 recon=8326.6172 kl=13.8449 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 15 step 200/700 total=8341.9658 recon=8326.7539 kl=15.2118 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 15 step 300/700 total=8325.2412 recon=8311.0527 kl=14.1885 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 15 step 400/700 total=8328.3975 recon=8314.1934 kl=14.2044 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 15 step 500/700 total=8336.5156 recon=8322.5801 kl=13.9354 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 15 step 600/700 total=8329.0566 recon=8314.0879 kl=14.9685 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch16\n",
      "[train] epoch 16 step 0/700 total=8330.1455 recon=8315.5342 kl=14.6109 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 16 step 100/700 total=8329.7275 recon=8316.0215 kl=13.7059 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 16 step 200/700 total=8336.3652 recon=8322.1094 kl=14.2557 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 16 step 300/700 total=8342.8906 recon=8329.2803 kl=13.6104 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 16 step 400/700 total=8317.9082 recon=8301.9521 kl=15.9562 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 16 step 500/700 total=8337.5605 recon=8323.2939 kl=14.2666 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 16 step 600/700 total=8336.9375 recon=8321.6582 kl=15.2791 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch17\n",
      "[train] epoch 17 step 0/700 total=8344.2705 recon=8330.0557 kl=14.2144 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 17 step 100/700 total=8326.8945 recon=8312.1982 kl=14.6962 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 17 step 200/700 total=8323.8525 recon=8308.1533 kl=15.6995 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 17 step 300/700 total=8324.3008 recon=8309.7451 kl=14.5556 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 17 step 400/700 total=8330.3770 recon=8315.5225 kl=14.8543 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 17 step 500/700 total=8339.5322 recon=8324.8438 kl=14.6884 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 17 step 600/700 total=8343.1348 recon=8328.0527 kl=15.0822 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch18\n",
      "[train] epoch 18 step 0/700 total=8342.5146 recon=8327.9854 kl=14.5292 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 18 step 100/700 total=8339.9678 recon=8326.0771 kl=13.8908 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 18 step 200/700 total=8331.3838 recon=8317.4033 kl=13.9802 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 18 step 300/700 total=8337.3096 recon=8323.0996 kl=14.2098 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 18 step 400/700 total=8329.8350 recon=8314.7861 kl=15.0491 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 18 step 500/700 total=8324.6846 recon=8310.4795 kl=14.2050 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 18 step 600/700 total=8329.0674 recon=8314.3438 kl=14.7236 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch19\n",
      "[train] epoch 19 step 0/700 total=8331.8428 recon=8317.4365 kl=14.4065 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 19 step 100/700 total=8323.6553 recon=8309.0752 kl=14.5799 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 19 step 200/700 total=8330.5479 recon=8316.2451 kl=14.3030 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 19 step 300/700 total=8341.5664 recon=8326.7246 kl=14.8419 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 19 step 400/700 total=8330.8320 recon=8316.9121 kl=13.9201 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 19 step 500/700 total=8333.8311 recon=8319.2568 kl=14.5743 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 19 step 600/700 total=8338.9893 recon=8323.6895 kl=15.2998 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch20\n",
      "[train] epoch 20 step 0/700 total=8338.0586 recon=8323.6670 kl=14.3920 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 20 step 100/700 total=8338.8066 recon=8324.9189 kl=13.8874 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 20 step 200/700 total=8325.3799 recon=8310.7266 kl=14.6535 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 20 step 300/700 total=8328.0586 recon=8312.2627 kl=15.7955 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 20 step 400/700 total=8342.5947 recon=8328.6582 kl=13.9362 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 20 step 500/700 total=8318.3525 recon=8303.9736 kl=14.3787 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 20 step 600/700 total=8319.5967 recon=8305.3652 kl=14.2317 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch21\n",
      "[train] epoch 21 step 0/700 total=8332.4111 recon=8318.0967 kl=14.3142 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 21 step 100/700 total=8325.0488 recon=8309.5088 kl=15.5402 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 21 step 200/700 total=8333.8711 recon=8318.8262 kl=15.0447 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 21 step 300/700 total=8346.1953 recon=8331.5000 kl=14.6953 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 21 step 400/700 total=8329.1748 recon=8315.2959 kl=13.8785 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 21 step 500/700 total=8334.9014 recon=8320.1992 kl=14.7018 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 21 step 600/700 total=8330.0781 recon=8316.3262 kl=13.7521 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch22\n",
      "[train] epoch 22 step 0/700 total=8335.1738 recon=8320.7051 kl=14.4686 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 22 step 100/700 total=8340.5713 recon=8326.1787 kl=14.3927 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 22 step 200/700 total=8337.2373 recon=8322.4189 kl=14.8182 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 22 step 300/700 total=8334.0391 recon=8319.9707 kl=14.0687 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 22 step 400/700 total=8337.1904 recon=8322.9971 kl=14.1934 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 22 step 500/700 total=8332.6504 recon=8318.4541 kl=14.1966 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 22 step 600/700 total=8331.9199 recon=8317.7236 kl=14.1965 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch23\n",
      "[train] epoch 23 step 0/700 total=8333.5967 recon=8318.7246 kl=14.8723 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 23 step 100/700 total=8317.6602 recon=8302.7510 kl=14.9097 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 23 step 200/700 total=8337.8379 recon=8323.0117 kl=14.8265 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 23 step 300/700 total=8345.4873 recon=8331.2637 kl=14.2238 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 23 step 400/700 total=8340.3018 recon=8325.2715 kl=15.0306 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 23 step 500/700 total=8325.8994 recon=8311.1250 kl=14.7745 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 23 step 600/700 total=8333.6465 recon=8319.7979 kl=13.8490 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch24\n",
      "[train] epoch 24 step 0/700 total=8329.2842 recon=8314.4590 kl=14.8251 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 24 step 100/700 total=8338.8174 recon=8324.1045 kl=14.7129 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 24 step 200/700 total=8330.2568 recon=8315.0469 kl=15.2096 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 24 step 300/700 total=8338.0732 recon=8323.2178 kl=14.8554 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 24 step 400/700 total=8326.8232 recon=8312.1436 kl=14.6796 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 24 step 500/700 total=8334.8965 recon=8319.9365 kl=14.9599 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 24 step 600/700 total=8338.8906 recon=8324.6631 kl=14.2273 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch25\n",
      "[train] epoch 25 step 0/700 total=8324.1328 recon=8308.6133 kl=15.5198 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 25 step 100/700 total=8336.3154 recon=8321.7256 kl=14.5899 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 25 step 200/700 total=8323.7402 recon=8309.2754 kl=14.4651 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 25 step 300/700 total=8341.7002 recon=8326.6387 kl=15.0615 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 25 step 400/700 total=8336.8662 recon=8321.5244 kl=15.3422 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 25 step 500/700 total=8324.1875 recon=8309.1094 kl=15.0782 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 25 step 600/700 total=8327.0586 recon=8311.8359 kl=15.2222 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch26\n",
      "[train] epoch 26 step 0/700 total=8321.5479 recon=8305.7354 kl=15.8123 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 26 step 100/700 total=8329.3301 recon=8314.8877 kl=14.4424 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 26 step 200/700 total=8322.6924 recon=8307.6396 kl=15.0530 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 26 step 300/700 total=8332.7383 recon=8317.5068 kl=15.2314 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 26 step 400/700 total=8341.8408 recon=8327.3936 kl=14.4469 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 26 step 500/700 total=8342.4805 recon=8326.8418 kl=15.6386 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 26 step 600/700 total=8334.2920 recon=8318.8672 kl=15.4246 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch27\n",
      "[train] epoch 27 step 0/700 total=8349.6719 recon=8335.0889 kl=14.5833 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 27 step 100/700 total=8336.8926 recon=8321.0781 kl=15.8145 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 27 step 200/700 total=8334.6426 recon=8319.8633 kl=14.7797 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 27 step 300/700 total=8325.4209 recon=8311.0039 kl=14.4168 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 27 step 400/700 total=8340.7949 recon=8326.5889 kl=14.2059 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 27 step 500/700 total=8332.6182 recon=8318.3213 kl=14.2965 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 27 step 600/700 total=8342.5527 recon=8327.1045 kl=15.4480 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch28\n",
      "[train] epoch 28 step 0/700 total=8333.0762 recon=8317.5684 kl=15.5077 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 28 step 100/700 total=8319.2803 recon=8303.7939 kl=15.4862 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 28 step 200/700 total=8337.2500 recon=8322.0420 kl=15.2077 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 28 step 300/700 total=8336.9404 recon=8322.0361 kl=14.9047 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 28 step 400/700 total=8327.8223 recon=8312.4326 kl=15.3894 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 28 step 500/700 total=8324.6328 recon=8308.7275 kl=15.9054 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 28 step 600/700 total=8335.3262 recon=8320.5059 kl=14.8203 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch29\n",
      "[train] epoch 29 step 0/700 total=8319.7891 recon=8303.9688 kl=15.8208 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 29 step 100/700 total=8325.7002 recon=8309.5000 kl=16.2006 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 29 step 200/700 total=8327.2793 recon=8311.4326 kl=15.8472 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 29 step 300/700 total=8327.6240 recon=8311.7461 kl=15.8776 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 29 step 400/700 total=8316.3584 recon=8300.5713 kl=15.7869 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 29 step 500/700 total=8319.8281 recon=8304.7656 kl=15.0625 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 29 step 600/700 total=8332.9277 recon=8318.1084 kl=14.8198 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch30\n",
      "[train] epoch 30 step 0/700 total=8336.7686 recon=8322.0420 kl=14.7270 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 30 step 100/700 total=8337.4111 recon=8322.8398 kl=14.5710 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 30 step 200/700 total=8325.6699 recon=8310.0781 kl=15.5918 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 30 step 300/700 total=8332.0576 recon=8316.9648 kl=15.0930 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 30 step 400/700 total=8316.8721 recon=8301.2490 kl=15.6228 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 30 step 500/700 total=8341.5469 recon=8326.1973 kl=15.3497 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 30 step 600/700 total=8323.9326 recon=8308.4961 kl=15.4364 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch31\n",
      "[train] epoch 31 step 0/700 total=8338.9248 recon=8323.7715 kl=15.1537 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 31 step 100/700 total=8332.5166 recon=8317.5332 kl=14.9830 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 31 step 200/700 total=8333.1934 recon=8318.1719 kl=15.0219 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 31 step 300/700 total=8323.0322 recon=8307.3115 kl=15.7207 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 31 step 400/700 total=8335.2197 recon=8320.3379 kl=14.8820 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 31 step 500/700 total=8332.8555 recon=8317.5469 kl=15.3083 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 31 step 600/700 total=8340.6367 recon=8325.7441 kl=14.8925 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch32\n",
      "[train] epoch 32 step 0/700 total=8342.5850 recon=8327.9023 kl=14.6829 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 32 step 100/700 total=8335.6709 recon=8320.1582 kl=15.5126 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 32 step 200/700 total=8322.6650 recon=8306.8359 kl=15.8287 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 32 step 300/700 total=8343.3467 recon=8328.4795 kl=14.8670 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 32 step 400/700 total=8325.4658 recon=8308.7695 kl=16.6960 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 32 step 500/700 total=8333.5117 recon=8318.3789 kl=15.1329 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 32 step 600/700 total=8320.3164 recon=8304.5840 kl=15.7325 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch33\n",
      "[train] epoch 33 step 0/700 total=8330.9131 recon=8315.1260 kl=15.7872 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 33 step 100/700 total=8332.9521 recon=8317.1621 kl=15.7903 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 33 step 200/700 total=8322.0234 recon=8305.8223 kl=16.2009 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 33 step 300/700 total=8335.5000 recon=8319.4961 kl=16.0037 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 33 step 400/700 total=8336.6484 recon=8320.6377 kl=16.0107 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 33 step 500/700 total=8325.4277 recon=8309.8838 kl=15.5439 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 33 step 600/700 total=8343.6396 recon=8328.5020 kl=15.1373 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch34\n",
      "[train] epoch 34 step 0/700 total=8326.5762 recon=8311.3574 kl=15.2187 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 34 step 100/700 total=8330.1221 recon=8315.2588 kl=14.8636 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 34 step 200/700 total=8334.6016 recon=8319.6006 kl=15.0006 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 34 step 300/700 total=8319.3428 recon=8303.7012 kl=15.6416 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 34 step 400/700 total=8338.0781 recon=8322.7090 kl=15.3695 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 34 step 500/700 total=8332.4004 recon=8316.2803 kl=16.1201 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 34 step 600/700 total=8331.8838 recon=8315.6797 kl=16.2039 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch35\n",
      "[train] epoch 35 step 0/700 total=8331.3291 recon=8314.6611 kl=16.6683 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 35 step 100/700 total=8327.1680 recon=8311.7266 kl=15.4416 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 35 step 200/700 total=8323.6738 recon=8307.4395 kl=16.2347 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 35 step 300/700 total=8339.7676 recon=8324.8223 kl=14.9453 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 35 step 400/700 total=8334.9102 recon=8319.3877 kl=15.5222 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 35 step 500/700 total=8326.4932 recon=8311.1201 kl=15.3730 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 35 step 600/700 total=8343.4941 recon=8327.6807 kl=15.8133 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch36\n",
      "[train] epoch 36 step 0/700 total=8321.9541 recon=8306.5361 kl=15.4177 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 36 step 100/700 total=8333.8867 recon=8318.7227 kl=15.1643 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 36 step 200/700 total=8342.2070 recon=8327.4971 kl=14.7101 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 36 step 300/700 total=8331.1133 recon=8316.2832 kl=14.8302 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 36 step 400/700 total=8330.4297 recon=8314.8418 kl=15.5876 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 36 step 500/700 total=8333.2578 recon=8318.0889 kl=15.1693 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 36 step 600/700 total=8321.8047 recon=8306.1201 kl=15.6845 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch37\n",
      "[train] epoch 37 step 0/700 total=8338.4395 recon=8323.4043 kl=15.0350 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 37 step 100/700 total=8318.3682 recon=8303.4707 kl=14.8972 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 37 step 200/700 total=8341.7539 recon=8326.6445 kl=15.1092 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 37 step 300/700 total=8329.7109 recon=8314.4434 kl=15.2680 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 37 step 400/700 total=8326.2061 recon=8310.1982 kl=16.0076 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 37 step 500/700 total=8334.8613 recon=8318.4688 kl=16.3924 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 37 step 600/700 total=8341.4922 recon=8325.5361 kl=15.9562 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch38\n",
      "[train] epoch 38 step 0/700 total=8323.1660 recon=8307.4932 kl=15.6731 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 38 step 100/700 total=8322.5205 recon=8306.7910 kl=15.7296 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 38 step 200/700 total=8330.3350 recon=8314.3066 kl=16.0286 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 38 step 300/700 total=8342.8936 recon=8327.7949 kl=15.0988 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 38 step 400/700 total=8326.7266 recon=8311.9990 kl=14.7278 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 38 step 500/700 total=8325.5771 recon=8310.1611 kl=15.4165 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 38 step 600/700 total=8321.3721 recon=8305.3447 kl=16.0277 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch39\n",
      "[train] epoch 39 step 0/700 total=8332.6055 recon=8316.8037 kl=15.8013 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 39 step 100/700 total=8321.0479 recon=8305.3965 kl=15.6517 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 39 step 200/700 total=8342.2012 recon=8326.8721 kl=15.3288 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 39 step 300/700 total=8328.2344 recon=8312.2969 kl=15.9371 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 39 step 400/700 total=8330.8975 recon=8315.9365 kl=14.9612 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 39 step 500/700 total=8332.6514 recon=8316.6816 kl=15.9695 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 39 step 600/700 total=8339.8691 recon=8325.0088 kl=14.8602 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch40\n",
      "[train] epoch 40 step 0/700 total=8340.8242 recon=8325.7412 kl=15.0833 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 40 step 100/700 total=8324.2021 recon=8308.6748 kl=15.5274 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 40 step 200/700 total=8343.1748 recon=8327.9121 kl=15.2631 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 40 step 300/700 total=8325.7881 recon=8309.1523 kl=16.6359 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 40 step 400/700 total=8331.0303 recon=8316.6426 kl=14.3877 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 40 step 500/700 total=8334.6318 recon=8319.8564 kl=14.7750 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 40 step 600/700 total=8319.9180 recon=8304.5469 kl=15.3711 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch41\n",
      "[train] epoch 41 step 0/700 total=8336.0566 recon=8321.0674 kl=14.9895 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 41 step 100/700 total=8338.5029 recon=8322.6113 kl=15.8917 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 41 step 200/700 total=8337.0352 recon=8322.1504 kl=14.8852 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 41 step 300/700 total=8320.5010 recon=8304.5244 kl=15.9762 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 41 step 400/700 total=8325.0723 recon=8309.5996 kl=15.4723 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 41 step 500/700 total=8329.7842 recon=8314.8818 kl=14.9020 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 41 step 600/700 total=8318.1426 recon=8301.1562 kl=16.9865 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch42\n",
      "[train] epoch 42 step 0/700 total=8325.9453 recon=8309.7783 kl=16.1668 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 42 step 100/700 total=8324.5576 recon=8308.6445 kl=15.9134 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 42 step 200/700 total=8323.7373 recon=8307.5000 kl=16.2372 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 42 step 300/700 total=8319.7500 recon=8303.3516 kl=16.3982 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 42 step 400/700 total=8329.3877 recon=8313.4727 kl=15.9151 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 42 step 500/700 total=8324.5352 recon=8308.2471 kl=16.2882 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 42 step 600/700 total=8324.5371 recon=8308.7207 kl=15.8162 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch43\n",
      "[train] epoch 43 step 0/700 total=8336.2246 recon=8321.2871 kl=14.9375 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 43 step 100/700 total=8319.9883 recon=8304.4238 kl=15.5647 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 43 step 200/700 total=8333.2148 recon=8317.4961 kl=15.7185 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 43 step 300/700 total=8337.5205 recon=8321.6758 kl=15.8448 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 43 step 400/700 total=8334.3936 recon=8318.9131 kl=15.4804 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 43 step 500/700 total=8328.7500 recon=8312.7314 kl=16.0190 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 43 step 600/700 total=8341.8887 recon=8326.8057 kl=15.0835 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch44\n",
      "[train] epoch 44 step 0/700 total=8320.6660 recon=8305.9365 kl=14.7295 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 44 step 100/700 total=8332.0713 recon=8316.6650 kl=15.4062 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 44 step 200/700 total=8333.2334 recon=8316.9199 kl=16.3132 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 44 step 300/700 total=8324.1729 recon=8308.2539 kl=15.9185 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 44 step 400/700 total=8339.3799 recon=8323.9443 kl=15.4351 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 44 step 500/700 total=8330.8936 recon=8315.2031 kl=15.6900 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 44 step 600/700 total=8340.4336 recon=8325.3789 kl=15.0548 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch45\n",
      "[train] epoch 45 step 0/700 total=8339.6123 recon=8324.2920 kl=15.3204 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 45 step 100/700 total=8330.5879 recon=8314.6875 kl=15.9000 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 45 step 200/700 total=8339.7119 recon=8324.0488 kl=15.6627 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 45 step 300/700 total=8326.9482 recon=8310.9209 kl=16.0276 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 45 step 400/700 total=8324.6816 recon=8308.9834 kl=15.6982 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 45 step 500/700 total=8323.9414 recon=8307.5176 kl=16.4241 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 45 step 600/700 total=8328.2510 recon=8312.4600 kl=15.7906 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch46\n",
      "[train] epoch 46 step 0/700 total=8327.9326 recon=8312.2334 kl=15.6996 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 46 step 100/700 total=8327.9219 recon=8311.8594 kl=16.0620 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 46 step 200/700 total=8323.3506 recon=8308.1865 kl=15.1636 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 46 step 300/700 total=8329.3496 recon=8313.6152 kl=15.7348 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 46 step 400/700 total=8330.2617 recon=8313.9482 kl=16.3132 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 46 step 500/700 total=8341.8945 recon=8326.6045 kl=15.2900 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 46 step 600/700 total=8323.7363 recon=8308.3242 kl=15.4118 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch47\n",
      "[train] epoch 47 step 0/700 total=8319.1953 recon=8303.3770 kl=15.8182 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 47 step 100/700 total=8324.7842 recon=8308.5381 kl=16.2462 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 47 step 200/700 total=8330.3662 recon=8314.5439 kl=15.8220 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 47 step 300/700 total=8338.1650 recon=8322.1396 kl=16.0255 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 47 step 400/700 total=8333.7920 recon=8318.0049 kl=15.7869 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 47 step 500/700 total=8326.9775 recon=8311.6523 kl=15.3253 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 47 step 600/700 total=8335.4072 recon=8319.9619 kl=15.4451 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch48\n",
      "[train] epoch 48 step 0/700 total=8332.6807 recon=8317.0801 kl=15.6005 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 48 step 100/700 total=8338.4697 recon=8322.8428 kl=15.6267 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 48 step 200/700 total=8342.2012 recon=8327.1338 kl=15.0674 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 48 step 300/700 total=8340.2803 recon=8325.1826 kl=15.0975 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 48 step 400/700 total=8333.6152 recon=8317.2383 kl=16.3770 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 48 step 500/700 total=8343.2461 recon=8327.9229 kl=15.3236 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 48 step 600/700 total=8329.0176 recon=8313.8623 kl=15.1550 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch49\n",
      "[train] epoch 49 step 0/700 total=8331.6182 recon=8316.3818 kl=15.2368 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 49 step 100/700 total=8330.0586 recon=8314.2920 kl=15.7667 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 49 step 200/700 total=8339.9746 recon=8324.6592 kl=15.3153 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 49 step 300/700 total=8329.4170 recon=8314.1992 kl=15.2177 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 49 step 400/700 total=8314.4150 recon=8298.2012 kl=16.2135 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 49 step 500/700 total=8327.8838 recon=8312.2686 kl=15.6150 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 49 step 600/700 total=8334.9297 recon=8319.1064 kl=15.8234 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch50\n",
      "[train] epoch 50 step 0/700 total=8323.8916 recon=8307.8203 kl=16.0716 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 50 step 100/700 total=8332.6289 recon=8316.9053 kl=15.7238 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 50 step 200/700 total=8336.2393 recon=8320.3223 kl=15.9166 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 50 step 300/700 total=8333.8418 recon=8318.0107 kl=15.8313 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 50 step 400/700 total=8335.2100 recon=8319.2871 kl=15.9225 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 50 step 500/700 total=8323.3535 recon=8306.8936 kl=16.4601 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 50 step 600/700 total=8332.0566 recon=8316.7559 kl=15.3010 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch51\n",
      "[train] epoch 51 step 0/700 total=8330.1318 recon=8314.6562 kl=15.4754 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 51 step 100/700 total=8330.7266 recon=8315.5000 kl=15.2263 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 51 step 200/700 total=8333.3633 recon=8317.4053 kl=15.9579 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 51 step 300/700 total=8323.2012 recon=8306.8115 kl=16.3895 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 51 step 400/700 total=8322.7617 recon=8306.5664 kl=16.1949 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 51 step 500/700 total=8330.9590 recon=8315.0498 kl=15.9094 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 51 step 600/700 total=8336.6152 recon=8320.7920 kl=15.8233 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch52\n",
      "[train] epoch 52 step 0/700 total=8322.4307 recon=8306.9219 kl=15.5084 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 52 step 100/700 total=8329.4326 recon=8313.2568 kl=16.1756 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 52 step 200/700 total=8324.5322 recon=8308.5957 kl=15.9361 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 52 step 300/700 total=8324.4844 recon=8308.8936 kl=15.5905 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 52 step 400/700 total=8332.2598 recon=8316.0068 kl=16.2530 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 52 step 500/700 total=8317.2695 recon=8301.3975 kl=15.8721 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 52 step 600/700 total=8319.9707 recon=8304.1377 kl=15.8335 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch53\n",
      "[train] epoch 53 step 0/700 total=8329.7520 recon=8313.4775 kl=16.2739 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 53 step 100/700 total=8332.2100 recon=8316.6426 kl=15.5675 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 53 step 200/700 total=8333.0977 recon=8317.4160 kl=15.6816 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 53 step 300/700 total=8340.4238 recon=8324.8311 kl=15.5923 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 53 step 400/700 total=8326.1143 recon=8309.2842 kl=16.8298 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 53 step 500/700 total=8334.9541 recon=8319.1748 kl=15.7792 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 53 step 600/700 total=8333.1416 recon=8317.1611 kl=15.9809 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch54\n",
      "[train] epoch 54 step 0/700 total=8326.2031 recon=8310.1621 kl=16.0406 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 54 step 100/700 total=8324.1533 recon=8308.0674 kl=16.0860 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 54 step 200/700 total=8319.2734 recon=8302.9824 kl=16.2908 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 54 step 300/700 total=8324.6172 recon=8308.9688 kl=15.6480 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 54 step 400/700 total=8331.8799 recon=8315.9912 kl=15.8882 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 54 step 500/700 total=8330.2285 recon=8314.5244 kl=15.7044 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 54 step 600/700 total=8337.7617 recon=8322.3594 kl=15.4020 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch55\n",
      "[train] epoch 55 step 0/700 total=8315.9980 recon=8299.5098 kl=16.4888 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 55 step 100/700 total=8328.3896 recon=8312.0957 kl=16.2938 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 55 step 200/700 total=8323.6543 recon=8308.1016 kl=15.5527 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 55 step 300/700 total=8327.2578 recon=8310.7939 kl=16.4641 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 55 step 400/700 total=8323.9883 recon=8307.9795 kl=16.0088 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 55 step 500/700 total=8337.3838 recon=8321.3984 kl=15.9850 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 55 step 600/700 total=8335.9492 recon=8319.5801 kl=16.3689 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch56\n",
      "[train] epoch 56 step 0/700 total=8317.6250 recon=8301.0303 kl=16.5952 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 56 step 100/700 total=8339.9062 recon=8324.8848 kl=15.0210 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 56 step 200/700 total=8330.2275 recon=8314.2373 kl=15.9904 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 56 step 300/700 total=8334.4258 recon=8318.5381 kl=15.8875 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 56 step 400/700 total=8339.6416 recon=8323.8389 kl=15.8027 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 56 step 500/700 total=8318.4785 recon=8302.4912 kl=15.9871 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 56 step 600/700 total=8326.7002 recon=8310.4658 kl=16.2345 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch57\n",
      "[train] epoch 57 step 0/700 total=8338.7197 recon=8323.0195 kl=15.7002 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 57 step 100/700 total=8329.1875 recon=8313.7764 kl=15.4112 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 57 step 200/700 total=8323.8867 recon=8307.5518 kl=16.3346 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 57 step 300/700 total=8330.6182 recon=8315.0508 kl=15.5676 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 57 step 400/700 total=8329.6211 recon=8312.9531 kl=16.6679 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 57 step 500/700 total=8334.2500 recon=8317.6719 kl=16.5779 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 57 step 600/700 total=8322.3340 recon=8306.0947 kl=16.2397 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch58\n",
      "[train] epoch 58 step 0/700 total=8314.8828 recon=8298.3613 kl=16.5216 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 58 step 100/700 total=8332.6113 recon=8316.5850 kl=16.0266 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 58 step 200/700 total=8322.4824 recon=8305.4160 kl=17.0663 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 58 step 300/700 total=8321.3994 recon=8304.8760 kl=16.5239 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 58 step 400/700 total=8339.0283 recon=8323.1182 kl=15.9102 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 58 step 500/700 total=8335.9365 recon=8320.4785 kl=15.4583 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 58 step 600/700 total=8332.9209 recon=8317.1523 kl=15.7682 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch59\n",
      "[train] epoch 59 step 0/700 total=8327.4688 recon=8311.0020 kl=16.4667 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 59 step 100/700 total=8328.1406 recon=8311.8535 kl=16.2870 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 59 step 200/700 total=8323.0312 recon=8306.0732 kl=16.9585 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 59 step 300/700 total=8320.3066 recon=8302.8652 kl=17.4415 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 59 step 400/700 total=8341.1006 recon=8325.2676 kl=15.8329 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 59 step 500/700 total=8341.3047 recon=8325.2188 kl=16.0863 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 59 step 600/700 total=8324.2500 recon=8307.8682 kl=16.3818 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch60\n",
      "[train] epoch 60 step 0/700 total=8343.0635 recon=8327.8750 kl=15.1887 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 60 step 100/700 total=8318.5977 recon=8301.8926 kl=16.7050 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 60 step 200/700 total=8339.2910 recon=8322.5557 kl=16.7355 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 60 step 300/700 total=8326.4648 recon=8310.6738 kl=15.7911 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 60 step 400/700 total=8327.5195 recon=8311.3896 kl=16.1300 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 60 step 500/700 total=8321.1680 recon=8304.9482 kl=16.2199 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 60 step 600/700 total=8328.2295 recon=8312.6045 kl=15.6250 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch61\n",
      "[train] epoch 61 step 0/700 total=8337.1348 recon=8320.7549 kl=16.3802 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 61 step 100/700 total=8328.9521 recon=8313.3252 kl=15.6272 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 61 step 200/700 total=8337.0684 recon=8321.4912 kl=15.5767 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 61 step 300/700 total=8325.5898 recon=8309.9316 kl=15.6578 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 61 step 400/700 total=8338.3701 recon=8322.6348 kl=15.7354 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 61 step 500/700 total=8317.5176 recon=8301.3711 kl=16.1468 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 61 step 600/700 total=8334.7227 recon=8318.8418 kl=15.8806 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch62\n",
      "[train] epoch 62 step 0/700 total=8340.8564 recon=8324.9648 kl=15.8913 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 62 step 100/700 total=8336.4609 recon=8321.1523 kl=15.3088 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 62 step 200/700 total=8320.6807 recon=8304.3623 kl=16.3188 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 62 step 300/700 total=8325.3320 recon=8308.6553 kl=16.6771 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 62 step 400/700 total=8324.2598 recon=8307.8857 kl=16.3737 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 62 step 500/700 total=8320.8945 recon=8304.8408 kl=16.0542 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 62 step 600/700 total=8317.5850 recon=8301.4121 kl=16.1726 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch63\n",
      "[train] epoch 63 step 0/700 total=8326.4834 recon=8309.8701 kl=16.6134 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 63 step 100/700 total=8338.1152 recon=8322.1133 kl=16.0017 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 63 step 200/700 total=8325.1650 recon=8309.0596 kl=16.1055 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 63 step 300/700 total=8322.7910 recon=8307.3770 kl=15.4143 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 63 step 400/700 total=8326.1924 recon=8309.5459 kl=16.6466 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 63 step 500/700 total=8317.0127 recon=8300.4326 kl=16.5801 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 63 step 600/700 total=8343.1123 recon=8327.1465 kl=15.9662 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch64\n",
      "[train] epoch 64 step 0/700 total=8320.8301 recon=8304.8271 kl=16.0029 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 64 step 100/700 total=8327.6475 recon=8311.5781 kl=16.0694 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 64 step 200/700 total=8336.4199 recon=8319.9375 kl=16.4823 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 64 step 300/700 total=8328.5254 recon=8312.5459 kl=15.9797 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 64 step 400/700 total=8327.6201 recon=8311.7070 kl=15.9127 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 64 step 500/700 total=8324.6133 recon=8307.8486 kl=16.7650 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 64 step 600/700 total=8341.7510 recon=8325.6855 kl=16.0651 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch65\n",
      "[train] epoch 65 step 0/700 total=8324.9082 recon=8308.5361 kl=16.3722 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 65 step 100/700 total=8329.8984 recon=8313.8496 kl=16.0493 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 65 step 200/700 total=8335.8506 recon=8319.4121 kl=16.4384 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 65 step 300/700 total=8327.4561 recon=8310.9502 kl=16.5060 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 65 step 400/700 total=8330.7393 recon=8315.2598 kl=15.4794 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 65 step 500/700 total=8323.2236 recon=8307.3242 kl=15.8992 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 65 step 600/700 total=8328.1445 recon=8312.0127 kl=16.1317 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch66\n",
      "[train] epoch 66 step 0/700 total=8330.5811 recon=8314.8799 kl=15.7014 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 66 step 100/700 total=8325.3477 recon=8309.0596 kl=16.2880 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 66 step 200/700 total=8323.7266 recon=8307.3506 kl=16.3764 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 66 step 300/700 total=8329.2207 recon=8312.8477 kl=16.3733 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 66 step 400/700 total=8317.2334 recon=8300.4375 kl=16.7959 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 66 step 500/700 total=8316.1318 recon=8299.6855 kl=16.4466 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 66 step 600/700 total=8334.7080 recon=8318.6953 kl=16.0124 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch67\n",
      "[train] epoch 67 step 0/700 total=8331.4375 recon=8315.4189 kl=16.0181 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 67 step 100/700 total=8325.3105 recon=8309.5107 kl=15.7996 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 67 step 200/700 total=8323.9053 recon=8307.9209 kl=15.9842 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 67 step 300/700 total=8332.7090 recon=8316.8662 kl=15.8431 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 67 step 400/700 total=8335.6855 recon=8319.4209 kl=16.2644 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 67 step 500/700 total=8338.5586 recon=8322.1318 kl=16.4267 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 67 step 600/700 total=8329.6377 recon=8313.5400 kl=16.0973 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch68\n",
      "[train] epoch 68 step 0/700 total=8330.7275 recon=8314.1982 kl=16.5289 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 68 step 100/700 total=8326.5742 recon=8310.2832 kl=16.2910 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 68 step 200/700 total=8329.5645 recon=8313.5840 kl=15.9806 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 68 step 300/700 total=8323.8652 recon=8307.8506 kl=16.0144 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 68 step 400/700 total=8330.7363 recon=8314.5186 kl=16.2174 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 68 step 500/700 total=8326.4746 recon=8310.2588 kl=16.2157 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 68 step 600/700 total=8319.7529 recon=8302.7725 kl=16.9804 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch69\n",
      "[train] epoch 69 step 0/700 total=8323.5820 recon=8307.4209 kl=16.1610 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 69 step 100/700 total=8317.9668 recon=8302.0615 kl=15.9049 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 69 step 200/700 total=8328.2139 recon=8311.8506 kl=16.3635 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 69 step 300/700 total=8328.8936 recon=8312.6133 kl=16.2806 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 69 step 400/700 total=8323.3584 recon=8307.4453 kl=15.9132 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 69 step 500/700 total=8313.2168 recon=8296.9150 kl=16.3016 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 69 step 600/700 total=8333.2871 recon=8317.5625 kl=15.7250 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch70\n",
      "[train] epoch 70 step 0/700 total=8337.7168 recon=8321.5674 kl=16.1491 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 70 step 100/700 total=8329.0225 recon=8312.7559 kl=16.2664 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 70 step 200/700 total=8327.4004 recon=8311.0801 kl=16.3204 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 70 step 300/700 total=8326.8467 recon=8311.0234 kl=15.8230 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 70 step 400/700 total=8339.2207 recon=8323.6035 kl=15.6173 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 70 step 500/700 total=8322.7920 recon=8305.9746 kl=16.8172 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 70 step 600/700 total=8321.0840 recon=8304.8457 kl=16.2378 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch71\n",
      "[train] epoch 71 step 0/700 total=8326.0947 recon=8309.7490 kl=16.3453 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 71 step 100/700 total=8322.3330 recon=8305.5283 kl=16.8042 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 71 step 200/700 total=8321.2832 recon=8304.8096 kl=16.4741 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 71 step 300/700 total=8334.1963 recon=8317.9219 kl=16.2739 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 71 step 400/700 total=8326.5654 recon=8310.5381 kl=16.0270 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 71 step 500/700 total=8328.3848 recon=8312.4424 kl=15.9426 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 71 step 600/700 total=8325.7393 recon=8309.5869 kl=16.1524 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch72\n",
      "[train] epoch 72 step 0/700 total=8339.3389 recon=8322.9619 kl=16.3772 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 72 step 100/700 total=8318.1875 recon=8301.0537 kl=17.1341 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 72 step 200/700 total=8336.4453 recon=8320.8076 kl=15.6381 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 72 step 300/700 total=8327.9189 recon=8311.8398 kl=16.0789 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 72 step 400/700 total=8326.1562 recon=8309.7773 kl=16.3785 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 72 step 500/700 total=8330.6416 recon=8314.5107 kl=16.1311 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 72 step 600/700 total=8327.4287 recon=8311.0000 kl=16.4292 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch73\n",
      "[train] epoch 73 step 0/700 total=8331.4990 recon=8316.0283 kl=15.4709 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 73 step 100/700 total=8330.2832 recon=8313.7158 kl=16.5674 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 73 step 200/700 total=8333.4600 recon=8317.2959 kl=16.1638 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 73 step 300/700 total=8341.7324 recon=8326.6162 kl=15.1165 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 73 step 400/700 total=8330.4141 recon=8314.6494 kl=15.7645 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 73 step 500/700 total=8329.3115 recon=8312.9141 kl=16.3976 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 73 step 600/700 total=8329.4512 recon=8313.0596 kl=16.3912 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch74\n",
      "[train] epoch 74 step 0/700 total=8341.7607 recon=8325.9922 kl=15.7682 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 74 step 100/700 total=8326.7363 recon=8310.1611 kl=16.5754 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 74 step 200/700 total=8335.4912 recon=8319.2852 kl=16.2062 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 74 step 300/700 total=8331.5840 recon=8315.7344 kl=15.8500 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 74 step 400/700 total=8336.1895 recon=8320.1934 kl=15.9957 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 74 step 500/700 total=8326.6113 recon=8310.4365 kl=16.1752 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 74 step 600/700 total=8327.2686 recon=8311.4609 kl=15.8074 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch75\n",
      "[train] epoch 75 step 0/700 total=8323.6035 recon=8307.4746 kl=16.1293 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 75 step 100/700 total=8321.8008 recon=8305.4102 kl=16.3907 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 75 step 200/700 total=8337.5625 recon=8321.0303 kl=16.5320 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 75 step 300/700 total=8333.5029 recon=8317.2549 kl=16.2481 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 75 step 400/700 total=8317.2598 recon=8300.8027 kl=16.4566 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 75 step 500/700 total=8322.6426 recon=8305.9463 kl=16.6961 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 75 step 600/700 total=8341.3945 recon=8325.7607 kl=15.6333 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch76\n",
      "[train] epoch 76 step 0/700 total=8328.1758 recon=8311.9453 kl=16.2309 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 76 step 100/700 total=8333.3828 recon=8316.9609 kl=16.4215 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 76 step 200/700 total=8335.5840 recon=8319.0928 kl=16.4908 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 76 step 300/700 total=8331.4414 recon=8315.2236 kl=16.2176 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 76 step 400/700 total=8326.5020 recon=8309.4014 kl=17.1002 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 76 step 500/700 total=8318.2383 recon=8302.1211 kl=16.1174 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 76 step 600/700 total=8327.0811 recon=8310.9434 kl=16.1377 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch77\n",
      "[train] epoch 77 step 0/700 total=8327.4668 recon=8310.8848 kl=16.5825 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 77 step 100/700 total=8332.7441 recon=8315.8359 kl=16.9078 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 77 step 200/700 total=8317.2285 recon=8300.9990 kl=16.2295 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 77 step 300/700 total=8341.0977 recon=8325.1221 kl=15.9757 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 77 step 400/700 total=8329.5928 recon=8313.4150 kl=16.1777 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 77 step 500/700 total=8335.4863 recon=8319.4512 kl=16.0353 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 77 step 600/700 total=8344.9736 recon=8329.2637 kl=15.7098 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch78\n",
      "[train] epoch 78 step 0/700 total=8328.8701 recon=8312.3115 kl=16.5585 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 78 step 100/700 total=8331.8174 recon=8315.4062 kl=16.4113 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 78 step 200/700 total=8320.6924 recon=8303.9297 kl=16.7625 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 78 step 300/700 total=8319.8506 recon=8303.3232 kl=16.5274 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 78 step 400/700 total=8330.7061 recon=8314.7744 kl=15.9318 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 78 step 500/700 total=8324.1357 recon=8307.4609 kl=16.6749 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 78 step 600/700 total=8313.1973 recon=8296.8125 kl=16.3848 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch79\n",
      "[train] epoch 79 step 0/700 total=8325.0400 recon=8308.6553 kl=16.3844 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 79 step 100/700 total=8329.4902 recon=8312.6572 kl=16.8326 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 79 step 200/700 total=8326.0449 recon=8310.0127 kl=16.0326 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 79 step 300/700 total=8317.0029 recon=8300.2969 kl=16.7056 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 79 step 400/700 total=8346.9277 recon=8330.9473 kl=15.9804 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 79 step 500/700 total=8329.2490 recon=8312.0117 kl=17.2373 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 79 step 600/700 total=8327.4785 recon=8311.2373 kl=16.2413 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch80\n",
      "[train] epoch 80 step 0/700 total=8326.4365 recon=8310.1084 kl=16.3279 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 80 step 100/700 total=8330.3945 recon=8313.5371 kl=16.8577 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 80 step 200/700 total=8329.6182 recon=8314.1270 kl=15.4911 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 80 step 300/700 total=8322.7598 recon=8306.5635 kl=16.1960 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 80 step 400/700 total=8331.2383 recon=8315.2314 kl=16.0072 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 80 step 500/700 total=8339.2451 recon=8323.2402 kl=16.0050 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 80 step 600/700 total=8325.6318 recon=8309.2451 kl=16.3868 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch81\n",
      "[train] epoch 81 step 0/700 total=8329.5615 recon=8313.2695 kl=16.2921 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 81 step 100/700 total=8326.9561 recon=8311.0449 kl=15.9112 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 81 step 200/700 total=8323.7070 recon=8307.1016 kl=16.6052 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 81 step 300/700 total=8317.1611 recon=8300.8301 kl=16.3307 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 81 step 400/700 total=8328.7100 recon=8311.9727 kl=16.7372 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 81 step 500/700 total=8332.3379 recon=8316.0908 kl=16.2473 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 81 step 600/700 total=8306.4482 recon=8289.9287 kl=16.5199 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch82\n",
      "[train] epoch 82 step 0/700 total=8334.0195 recon=8317.2500 kl=16.7697 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 82 step 100/700 total=8340.6162 recon=8325.0107 kl=15.6057 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 82 step 200/700 total=8336.8789 recon=8320.3936 kl=16.4852 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 82 step 300/700 total=8318.7715 recon=8302.4170 kl=16.3543 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 82 step 400/700 total=8330.9453 recon=8314.8486 kl=16.0970 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 82 step 500/700 total=8320.1113 recon=8303.6045 kl=16.5069 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 82 step 600/700 total=8330.5420 recon=8314.2764 kl=16.2655 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch83\n",
      "[train] epoch 83 step 0/700 total=8324.4795 recon=8308.0234 kl=16.4564 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 83 step 100/700 total=8332.2705 recon=8317.0342 kl=15.2361 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 83 step 200/700 total=8324.3643 recon=8307.4512 kl=16.9131 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 83 step 300/700 total=8332.0352 recon=8316.2920 kl=15.7431 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 83 step 400/700 total=8333.5312 recon=8317.0205 kl=16.5109 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 83 step 500/700 total=8319.4033 recon=8302.8398 kl=16.5637 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 83 step 600/700 total=8322.0498 recon=8305.3926 kl=16.6574 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch84\n",
      "[train] epoch 84 step 0/700 total=8342.3057 recon=8326.9424 kl=15.3636 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 84 step 100/700 total=8338.4639 recon=8322.6973 kl=15.7666 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 84 step 200/700 total=8324.8887 recon=8308.3975 kl=16.4916 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 84 step 300/700 total=8324.6221 recon=8307.8906 kl=16.7317 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 84 step 400/700 total=8334.5850 recon=8318.2744 kl=16.3105 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 84 step 500/700 total=8339.2119 recon=8323.3086 kl=15.9035 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 84 step 600/700 total=8335.6924 recon=8319.4951 kl=16.1974 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch85\n",
      "[train] epoch 85 step 0/700 total=8324.9102 recon=8308.4795 kl=16.4308 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 85 step 100/700 total=8333.2568 recon=8317.1826 kl=16.0739 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 85 step 200/700 total=8324.4307 recon=8308.1377 kl=16.2930 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 85 step 300/700 total=8334.1104 recon=8318.0088 kl=16.1014 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 85 step 400/700 total=8323.7686 recon=8306.9561 kl=16.8130 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 85 step 500/700 total=8328.3438 recon=8312.2344 kl=16.1093 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 85 step 600/700 total=8338.8379 recon=8322.2520 kl=16.5860 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch86\n",
      "[train] epoch 86 step 0/700 total=8333.6553 recon=8317.6582 kl=15.9968 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 86 step 100/700 total=8336.9424 recon=8320.6533 kl=16.2888 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 86 step 200/700 total=8323.7305 recon=8307.4072 kl=16.3228 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 86 step 300/700 total=8332.6455 recon=8316.6836 kl=15.9621 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 86 step 400/700 total=8319.9297 recon=8303.0000 kl=16.9292 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 86 step 500/700 total=8319.0498 recon=8302.4316 kl=16.6178 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 86 step 600/700 total=8315.7568 recon=8299.2412 kl=16.5157 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch87\n",
      "[train] epoch 87 step 0/700 total=8323.9795 recon=8308.0449 kl=15.9345 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 87 step 100/700 total=8335.9219 recon=8319.1660 kl=16.7556 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 87 step 200/700 total=8319.2188 recon=8302.8262 kl=16.3928 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 87 step 300/700 total=8330.2910 recon=8313.4443 kl=16.8468 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 87 step 400/700 total=8325.5020 recon=8308.9258 kl=16.5765 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 87 step 500/700 total=8329.6709 recon=8314.0518 kl=15.6187 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 87 step 600/700 total=8332.2520 recon=8316.3320 kl=15.9204 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch88\n",
      "[train] epoch 88 step 0/700 total=8323.2148 recon=8306.6162 kl=16.5985 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 88 step 100/700 total=8334.9062 recon=8318.7363 kl=16.1697 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 88 step 200/700 total=8335.1885 recon=8318.3682 kl=16.8202 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 88 step 300/700 total=8316.8965 recon=8299.9297 kl=16.9666 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 88 step 400/700 total=8324.4961 recon=8307.9150 kl=16.5811 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 88 step 500/700 total=8324.7725 recon=8308.1846 kl=16.5876 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 88 step 600/700 total=8319.6396 recon=8302.4346 kl=17.2047 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch89\n",
      "[train] epoch 89 step 0/700 total=8331.7822 recon=8315.7344 kl=16.0474 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 89 step 100/700 total=8325.1318 recon=8308.2725 kl=16.8591 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 89 step 200/700 total=8328.7539 recon=8312.3936 kl=16.3607 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 89 step 300/700 total=8329.2529 recon=8312.6865 kl=16.5664 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 89 step 400/700 total=8317.2725 recon=8300.7656 kl=16.5064 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 89 step 500/700 total=8327.0527 recon=8310.1934 kl=16.8593 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 89 step 600/700 total=8349.6416 recon=8334.4053 kl=15.2364 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch90\n",
      "[train] epoch 90 step 0/700 total=8325.3789 recon=8309.1338 kl=16.2455 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 90 step 100/700 total=8330.4570 recon=8314.1104 kl=16.3465 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 90 step 200/700 total=8333.0557 recon=8316.4365 kl=16.6192 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 90 step 300/700 total=8334.7949 recon=8317.7471 kl=17.0482 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 90 step 400/700 total=8322.2559 recon=8306.0312 kl=16.2250 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 90 step 500/700 total=8328.0361 recon=8311.7275 kl=16.3087 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 90 step 600/700 total=8337.5596 recon=8321.2998 kl=16.2593 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch91\n",
      "[train] epoch 91 step 0/700 total=8327.3389 recon=8310.6719 kl=16.6666 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 91 step 100/700 total=8327.8506 recon=8311.4170 kl=16.4335 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 91 step 200/700 total=8318.0791 recon=8300.6250 kl=17.4544 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 91 step 300/700 total=8336.7148 recon=8320.8984 kl=15.8164 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 91 step 400/700 total=8321.9336 recon=8305.6875 kl=16.2457 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 91 step 500/700 total=8323.4209 recon=8307.5439 kl=15.8769 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 91 step 600/700 total=8316.9785 recon=8300.5127 kl=16.4654 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch92\n",
      "[train] epoch 92 step 0/700 total=8327.9346 recon=8311.8936 kl=16.0409 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 92 step 100/700 total=8333.6660 recon=8318.1826 kl=15.4835 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 92 step 200/700 total=8321.4492 recon=8305.0000 kl=16.4496 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 92 step 300/700 total=8328.1230 recon=8311.4219 kl=16.7008 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 92 step 400/700 total=8329.2832 recon=8312.5879 kl=16.6955 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 92 step 500/700 total=8323.2383 recon=8307.1631 kl=16.0751 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 92 step 600/700 total=8313.2119 recon=8296.7969 kl=16.4151 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch93\n",
      "[train] epoch 93 step 0/700 total=8327.6572 recon=8311.5137 kl=16.1440 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 93 step 100/700 total=8323.2100 recon=8306.7744 kl=16.4354 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 93 step 200/700 total=8315.7334 recon=8299.4248 kl=16.3084 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 93 step 300/700 total=8323.5176 recon=8307.3564 kl=16.1608 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 93 step 400/700 total=8320.9980 recon=8303.9902 kl=17.0081 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 93 step 500/700 total=8337.5898 recon=8320.6875 kl=16.9020 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 93 step 600/700 total=8342.6016 recon=8327.0762 kl=15.5250 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch94\n",
      "[train] epoch 94 step 0/700 total=8334.0742 recon=8318.0068 kl=16.0672 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 94 step 100/700 total=8335.6729 recon=8319.2285 kl=16.4440 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 94 step 200/700 total=8328.1709 recon=8311.3428 kl=16.8281 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 94 step 300/700 total=8323.8828 recon=8306.5996 kl=17.2834 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 94 step 400/700 total=8331.8867 recon=8315.0957 kl=16.7913 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 94 step 500/700 total=8328.9971 recon=8312.7637 kl=16.2338 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 94 step 600/700 total=8337.9717 recon=8321.9023 kl=16.0693 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch95\n",
      "[train] epoch 95 step 0/700 total=8312.2559 recon=8295.3770 kl=16.8789 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 95 step 100/700 total=8321.9365 recon=8305.4736 kl=16.4629 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 95 step 200/700 total=8315.6543 recon=8298.7822 kl=16.8723 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 95 step 300/700 total=8329.3701 recon=8312.6797 kl=16.6900 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 95 step 400/700 total=8321.5459 recon=8304.4365 kl=17.1096 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 95 step 500/700 total=8328.3398 recon=8311.8672 kl=16.4731 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 95 step 600/700 total=8329.4307 recon=8312.9375 kl=16.4929 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch96\n",
      "[train] epoch 96 step 0/700 total=8326.7275 recon=8309.6025 kl=17.1253 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 96 step 100/700 total=8335.7178 recon=8318.7061 kl=17.0114 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 96 step 200/700 total=8329.8779 recon=8313.7207 kl=16.1569 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 96 step 300/700 total=8323.0654 recon=8306.5615 kl=16.5038 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 96 step 400/700 total=8329.5771 recon=8313.2852 kl=16.2917 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 96 step 500/700 total=8329.9434 recon=8314.1201 kl=15.8228 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 96 step 600/700 total=8326.2588 recon=8309.4561 kl=16.8025 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch97\n",
      "[train] epoch 97 step 0/700 total=8317.2617 recon=8300.6045 kl=16.6576 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 97 step 100/700 total=8332.6455 recon=8316.5615 kl=16.0843 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 97 step 200/700 total=8335.4121 recon=8318.7275 kl=16.6842 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 97 step 300/700 total=8337.5771 recon=8321.3965 kl=16.1802 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 97 step 400/700 total=8314.1016 recon=8296.9922 kl=17.1093 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 97 step 500/700 total=8337.7900 recon=8321.3281 kl=16.4622 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 97 step 600/700 total=8333.8477 recon=8317.7090 kl=16.1388 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch98\n",
      "[train] epoch 98 step 0/700 total=8341.3887 recon=8325.7490 kl=15.6395 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 98 step 100/700 total=8323.1035 recon=8306.2139 kl=16.8896 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 98 step 200/700 total=8318.3330 recon=8301.1904 kl=17.1423 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 98 step 300/700 total=8341.4521 recon=8325.1816 kl=16.2703 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 98 step 400/700 total=8323.6240 recon=8306.9297 kl=16.6946 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 98 step 500/700 total=8316.4160 recon=8299.4434 kl=16.9729 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 98 step 600/700 total=8331.9600 recon=8315.1953 kl=16.7644 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch99\n",
      "[train] epoch 99 step 0/700 total=8324.5010 recon=8307.5371 kl=16.9640 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 99 step 100/700 total=8337.1094 recon=8320.6553 kl=16.4540 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 99 step 200/700 total=8332.5117 recon=8315.7119 kl=16.8001 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 99 step 300/700 total=8343.9570 recon=8327.7129 kl=16.2444 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 99 step 400/700 total=8322.9951 recon=8306.2139 kl=16.7816 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 99 step 500/700 total=8328.9219 recon=8312.1621 kl=16.7594 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 99 step 600/700 total=8322.4150 recon=8305.8525 kl=16.5628 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch100\n",
      "[train] epoch 100 step 0/700 total=8319.5645 recon=8302.7090 kl=16.8555 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 100 step 100/700 total=8332.1201 recon=8315.1162 kl=17.0042 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 100 step 200/700 total=8332.7412 recon=8317.0107 kl=15.7307 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 100 step 300/700 total=8334.8604 recon=8319.0244 kl=15.8357 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 100 step 400/700 total=8347.0195 recon=8331.0176 kl=16.0023 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 100 step 500/700 total=8337.7910 recon=8321.4082 kl=16.3827 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 100 step 600/700 total=8326.6533 recon=8310.1025 kl=16.5507 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n"
     ]
    }
   ],
   "source": [
    "from src.train import TrainConfig, fit, evaluate\n",
    "import torch.optim as optim\n",
    "\n",
    "cfg64 = TrainConfig(\n",
    "    epochs=100,\n",
    "    lr=1e-3,\n",
    "    beta_kl=1,\n",
    "    lam_color=0,\n",
    "    lam_shape=0,\n",
    "    lam_count=0,\n",
    "    recon_loss=\"bce_logits\",   # 你已确认 images 在 [0,1] bce_logits\\mse\\l1\n",
    "    use_amp=False,\n",
    "    log_every=100,\n",
    "    save_best=False,\n",
    "    ckpt_dir=\"../checkpoints\",\n",
    "    ckpt_name=\"revae_v64.pt\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "\n",
    "cfg128 = TrainConfig(\n",
    "    epochs=100,\n",
    "    lr=1e-3,\n",
    "    beta_kl=1,\n",
    "    lam_color=0,\n",
    "    lam_shape=0,\n",
    "    lam_count=0,\n",
    "    recon_loss=\"bce_logits\",   # 你已确认 images 在 [0,1] bce_logits\\mse\\l1\n",
    "    use_amp=False,\n",
    "    log_every=100,\n",
    "    save_best=False,\n",
    "    ckpt_dir=\"../checkpoints\",\n",
    "    ckpt_name=\"revae_v128.pt\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "\n",
    "optimizer64 = optim.Adam(revae_64.parameters(), lr=cfg64.lr)\n",
    "optimizer128 = optim.Adam(revae_128.parameters(), lr=cfg128.lr)\n",
    "\n",
    "# 若你还没做 val_dl，可以先 val_loader=None\n",
    "result = fit(revae_64, train_dl_64, optimizer64, cfg64, val_loader=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cca9023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1\n",
      "[train] epoch 1 step 0/700 total=37296.2734 recon=37295.9297 kl=0.3446 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 1 step 100/700 total=33578.3633 recon=33549.5586 kl=28.8055 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 1 step 200/700 total=33534.2773 recon=33511.2109 kl=23.0683 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 1 step 300/700 total=33467.2734 recon=33446.8828 kl=20.3925 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 1 step 400/700 total=33481.5586 recon=33460.2383 kl=21.3199 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 1 step 500/700 total=33453.1680 recon=33431.4961 kl=21.6711 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 1 step 600/700 total=33434.0195 recon=33412.0078 kl=22.0104 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch2\n",
      "[train] epoch 2 step 0/700 total=33442.7422 recon=33420.8203 kl=21.9226 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 2 step 100/700 total=33389.4414 recon=33366.0586 kl=23.3810 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 2 step 200/700 total=33419.0352 recon=33397.8984 kl=21.1386 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 2 step 300/700 total=33416.0000 recon=33394.0781 kl=21.9233 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 2 step 400/700 total=33379.6133 recon=33357.1523 kl=22.4614 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 2 step 500/700 total=33382.1211 recon=33360.5078 kl=21.6132 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 2 step 600/700 total=33297.3086 recon=33273.6797 kl=23.6289 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch3\n",
      "[train] epoch 3 step 0/700 total=33347.5742 recon=33323.9883 kl=23.5841 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 3 step 100/700 total=33332.4648 recon=33308.8906 kl=23.5761 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 3 step 200/700 total=33303.8242 recon=33279.7344 kl=24.0908 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 3 step 300/700 total=33344.5508 recon=33321.3867 kl=23.1659 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 3 step 400/700 total=33314.9414 recon=33291.3594 kl=23.5805 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 3 step 500/700 total=33294.1680 recon=33270.1328 kl=24.0359 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 3 step 600/700 total=33310.5156 recon=33286.7930 kl=23.7227 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch4\n",
      "[train] epoch 4 step 0/700 total=33396.4297 recon=33373.0625 kl=23.3660 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 4 step 100/700 total=33261.8555 recon=33237.6406 kl=24.2134 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 4 step 200/700 total=33374.7227 recon=33351.3906 kl=23.3339 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 4 step 300/700 total=33315.6719 recon=33291.8203 kl=23.8526 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 4 step 400/700 total=33320.9492 recon=33296.8359 kl=24.1116 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 4 step 500/700 total=33325.8828 recon=33301.6602 kl=24.2210 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 4 step 600/700 total=33290.8086 recon=33266.5156 kl=24.2927 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch5\n",
      "[train] epoch 5 step 0/700 total=33343.7031 recon=33319.6680 kl=24.0370 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 5 step 100/700 total=33346.3086 recon=33321.6445 kl=24.6636 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 5 step 200/700 total=33339.1250 recon=33314.8516 kl=24.2741 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 5 step 300/700 total=33314.1523 recon=33288.9023 kl=25.2500 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 5 step 400/700 total=33343.2227 recon=33319.2344 kl=23.9884 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 5 step 500/700 total=33324.0781 recon=33300.7227 kl=23.3572 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 5 step 600/700 total=33248.7422 recon=33224.0000 kl=24.7407 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch6\n",
      "[train] epoch 6 step 0/700 total=33301.4414 recon=33276.8281 kl=24.6133 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 6 step 100/700 total=33318.5391 recon=33294.0898 kl=24.4485 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 6 step 200/700 total=33350.3281 recon=33325.5859 kl=24.7424 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 6 step 300/700 total=33313.8750 recon=33290.2031 kl=23.6737 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 6 step 400/700 total=33243.2500 recon=33219.2461 kl=24.0054 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 6 step 500/700 total=33323.5586 recon=33298.4922 kl=25.0658 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 6 step 600/700 total=33323.0273 recon=33298.6602 kl=24.3685 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch7\n",
      "[train] epoch 7 step 0/700 total=33262.7031 recon=33237.1250 kl=25.5800 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 7 step 100/700 total=33318.5039 recon=33293.1680 kl=25.3374 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 7 step 200/700 total=33318.9141 recon=33292.4180 kl=26.4943 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 7 step 300/700 total=33302.6367 recon=33278.1641 kl=24.4744 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 7 step 400/700 total=33353.3633 recon=33328.4609 kl=24.9009 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 7 step 500/700 total=33334.6836 recon=33309.2773 kl=25.4048 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 7 step 600/700 total=33269.9531 recon=33245.6484 kl=24.3058 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch8\n",
      "[train] epoch 8 step 0/700 total=33314.0664 recon=33289.6875 kl=24.3796 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 8 step 100/700 total=33270.1953 recon=33246.5859 kl=23.6089 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 8 step 200/700 total=33321.0703 recon=33298.4297 kl=22.6395 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 8 step 300/700 total=33362.1445 recon=33337.2695 kl=24.8734 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 8 step 400/700 total=33304.0508 recon=33279.5234 kl=24.5254 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 8 step 500/700 total=33288.2383 recon=33262.2305 kl=26.0068 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 8 step 600/700 total=33300.3398 recon=33276.6797 kl=23.6606 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch9\n",
      "[train] epoch 9 step 0/700 total=33328.2656 recon=33304.4727 kl=23.7931 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 9 step 100/700 total=33300.6758 recon=33275.9531 kl=24.7244 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 9 step 200/700 total=33292.7617 recon=33267.8242 kl=24.9377 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 9 step 300/700 total=33314.8945 recon=33288.5898 kl=26.3032 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 9 step 400/700 total=33294.4102 recon=33268.8750 kl=25.5344 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 9 step 500/700 total=33230.1836 recon=33204.7852 kl=25.3982 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 9 step 600/700 total=33266.0820 recon=33240.8242 kl=25.2569 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch10\n",
      "[train] epoch 10 step 0/700 total=33271.5078 recon=33247.3164 kl=24.1930 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 10 step 100/700 total=33304.8750 recon=33279.1953 kl=25.6785 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 10 step 200/700 total=33268.8281 recon=33244.0547 kl=24.7722 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 10 step 300/700 total=33269.5000 recon=33244.6055 kl=24.8960 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 10 step 400/700 total=33253.7695 recon=33228.4648 kl=25.3050 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 10 step 500/700 total=33314.5859 recon=33289.0312 kl=25.5532 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 10 step 600/700 total=33292.1484 recon=33266.2188 kl=25.9304 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch11\n",
      "[train] epoch 11 step 0/700 total=33269.9609 recon=33243.7344 kl=26.2270 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 11 step 100/700 total=33289.5234 recon=33265.2344 kl=24.2907 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 11 step 200/700 total=33302.1484 recon=33276.3633 kl=25.7840 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 11 step 300/700 total=33278.6289 recon=33252.0898 kl=26.5392 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 11 step 400/700 total=33254.8359 recon=33228.6680 kl=26.1691 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 11 step 500/700 total=33302.2852 recon=33278.2148 kl=24.0698 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 11 step 600/700 total=33341.4062 recon=33315.4492 kl=25.9584 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch12\n",
      "[train] epoch 12 step 0/700 total=33283.5234 recon=33259.3711 kl=24.1507 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 12 step 100/700 total=33310.8359 recon=33284.5859 kl=26.2488 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 12 step 200/700 total=33297.7773 recon=33271.5273 kl=26.2506 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 12 step 300/700 total=33310.5000 recon=33284.9609 kl=25.5375 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 12 step 400/700 total=33280.4688 recon=33255.0977 kl=25.3727 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 12 step 500/700 total=33292.2695 recon=33265.2109 kl=27.0598 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 12 step 600/700 total=33267.0508 recon=33241.0391 kl=26.0113 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch13\n",
      "[train] epoch 13 step 0/700 total=33250.6328 recon=33225.3203 kl=25.3131 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 13 step 100/700 total=33282.6211 recon=33258.2031 kl=24.4189 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 13 step 200/700 total=33317.8594 recon=33291.9883 kl=25.8713 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 13 step 300/700 total=33223.5273 recon=33197.5312 kl=25.9943 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 13 step 400/700 total=33252.8359 recon=33226.8828 kl=25.9538 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 13 step 500/700 total=33277.4805 recon=33252.1133 kl=25.3667 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 13 step 600/700 total=33276.8516 recon=33251.2500 kl=25.6021 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch14\n",
      "[train] epoch 14 step 0/700 total=33309.0664 recon=33282.5508 kl=26.5152 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 14 step 100/700 total=33288.2891 recon=33262.8711 kl=25.4186 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 14 step 200/700 total=33377.3828 recon=33350.5039 kl=26.8771 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 14 step 300/700 total=33267.3828 recon=33240.9180 kl=26.4638 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 14 step 400/700 total=33316.3398 recon=33291.7227 kl=24.6176 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 14 step 500/700 total=33257.3945 recon=33231.6484 kl=25.7475 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 14 step 600/700 total=33314.6445 recon=33289.8594 kl=24.7838 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch15\n",
      "[train] epoch 15 step 0/700 total=33292.7305 recon=33267.2695 kl=25.4612 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 15 step 100/700 total=33343.0781 recon=33316.7812 kl=26.2957 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 15 step 200/700 total=33301.3008 recon=33276.4531 kl=24.8462 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 15 step 300/700 total=33218.5586 recon=33192.6680 kl=25.8911 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 15 step 400/700 total=33254.4727 recon=33228.7305 kl=25.7406 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 15 step 500/700 total=33323.4922 recon=33297.3164 kl=26.1764 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 15 step 600/700 total=33270.3438 recon=33244.5742 kl=25.7690 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch16\n",
      "[train] epoch 16 step 0/700 total=33249.6602 recon=33223.6484 kl=26.0103 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 16 step 100/700 total=33296.9453 recon=33269.5039 kl=27.4432 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 16 step 200/700 total=33270.1367 recon=33244.5195 kl=25.6179 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 16 step 300/700 total=33319.2031 recon=33291.4180 kl=27.7834 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 16 step 400/700 total=33274.1484 recon=33247.7852 kl=26.3626 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 16 step 500/700 total=33317.0039 recon=33290.3594 kl=26.6448 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 16 step 600/700 total=33245.4102 recon=33220.6523 kl=24.7583 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch17\n",
      "[train] epoch 17 step 0/700 total=33325.0898 recon=33298.5586 kl=26.5296 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 17 step 100/700 total=33286.1875 recon=33259.8320 kl=26.3539 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 17 step 200/700 total=33285.7422 recon=33258.6250 kl=27.1157 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 17 step 300/700 total=33228.8672 recon=33202.4180 kl=26.4502 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 17 step 400/700 total=33313.9102 recon=33287.7227 kl=26.1861 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 17 step 500/700 total=33260.0508 recon=33234.3555 kl=25.6934 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 17 step 600/700 total=33296.8242 recon=33269.7656 kl=27.0595 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch18\n",
      "[train] epoch 18 step 0/700 total=33263.5547 recon=33237.4648 kl=26.0914 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 18 step 100/700 total=33318.8047 recon=33291.3477 kl=27.4560 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 18 step 200/700 total=33285.3750 recon=33259.1328 kl=26.2428 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 18 step 300/700 total=33311.9805 recon=33285.3203 kl=26.6604 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 18 step 400/700 total=33294.2656 recon=33268.1328 kl=26.1337 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 18 step 500/700 total=33296.5703 recon=33270.3047 kl=26.2673 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 18 step 600/700 total=33258.0625 recon=33231.8398 kl=26.2223 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch19\n",
      "[train] epoch 19 step 0/700 total=33245.0352 recon=33219.7461 kl=25.2908 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 19 step 100/700 total=33236.3125 recon=33210.5273 kl=25.7843 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 19 step 200/700 total=33244.9062 recon=33219.0781 kl=25.8297 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 19 step 300/700 total=33287.5664 recon=33261.5234 kl=26.0440 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 19 step 400/700 total=33257.4961 recon=33230.9180 kl=26.5798 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 19 step 500/700 total=33284.1758 recon=33257.7031 kl=26.4709 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 19 step 600/700 total=33311.5273 recon=33284.1367 kl=27.3925 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch20\n",
      "[train] epoch 20 step 0/700 total=33271.6406 recon=33246.2109 kl=25.4289 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 20 step 100/700 total=33297.3242 recon=33270.7383 kl=26.5858 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 20 step 200/700 total=33248.0859 recon=33219.6484 kl=28.4371 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 20 step 300/700 total=33250.8633 recon=33224.4336 kl=26.4300 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 20 step 400/700 total=33363.4102 recon=33335.8984 kl=27.5128 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 20 step 500/700 total=33260.9023 recon=33234.3945 kl=26.5085 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 20 step 600/700 total=33288.5469 recon=33262.0195 kl=26.5270 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch21\n",
      "[train] epoch 21 step 0/700 total=33236.2734 recon=33210.1055 kl=26.1677 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 21 step 100/700 total=33327.8125 recon=33300.7305 kl=27.0836 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 21 step 200/700 total=33307.2539 recon=33280.7695 kl=26.4836 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 21 step 300/700 total=33298.6172 recon=33271.5430 kl=27.0739 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 21 step 400/700 total=33303.1211 recon=33276.7500 kl=26.3696 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 21 step 500/700 total=33293.3320 recon=33266.0898 kl=27.2420 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 21 step 600/700 total=33246.7500 recon=33219.0078 kl=27.7418 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch22\n",
      "[train] epoch 22 step 0/700 total=33343.2344 recon=33316.2773 kl=26.9562 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 22 step 100/700 total=33271.7695 recon=33244.4492 kl=27.3203 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 22 step 200/700 total=33256.4844 recon=33229.7734 kl=26.7100 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 22 step 300/700 total=33258.1758 recon=33230.6445 kl=27.5327 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 22 step 400/700 total=33214.4414 recon=33186.9609 kl=27.4795 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 22 step 500/700 total=33276.6641 recon=33250.1914 kl=26.4714 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 22 step 600/700 total=33330.4688 recon=33302.8711 kl=27.5958 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch23\n",
      "[train] epoch 23 step 0/700 total=33266.4648 recon=33240.1641 kl=26.2999 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 23 step 100/700 total=33259.5000 recon=33233.0352 kl=26.4661 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 23 step 200/700 total=33260.3594 recon=33233.4609 kl=26.8968 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 23 step 300/700 total=33277.1211 recon=33250.1953 kl=26.9253 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 23 step 400/700 total=33226.3203 recon=33200.2109 kl=26.1087 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 23 step 500/700 total=33253.4297 recon=33226.1055 kl=27.3234 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 23 step 600/700 total=33284.6484 recon=33257.1367 kl=27.5107 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch24\n",
      "[train] epoch 24 step 0/700 total=33333.9844 recon=33306.3906 kl=27.5951 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 24 step 100/700 total=33266.6133 recon=33239.3125 kl=27.3010 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 24 step 200/700 total=33312.3516 recon=33285.0938 kl=27.2562 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 24 step 300/700 total=33342.9336 recon=33316.1484 kl=26.7864 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 24 step 400/700 total=33270.3477 recon=33242.6328 kl=27.7165 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 24 step 500/700 total=33253.6367 recon=33225.7305 kl=27.9066 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 24 step 600/700 total=33274.6211 recon=33246.1680 kl=28.4539 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch25\n",
      "[train] epoch 25 step 0/700 total=33268.0469 recon=33240.6602 kl=27.3851 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 25 step 100/700 total=33291.4141 recon=33264.1406 kl=27.2752 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 25 step 200/700 total=33242.4062 recon=33215.2656 kl=27.1423 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 25 step 300/700 total=33278.2852 recon=33250.1094 kl=28.1767 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 25 step 400/700 total=33268.5938 recon=33242.2031 kl=26.3919 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 25 step 500/700 total=33267.2070 recon=33240.9336 kl=26.2725 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 25 step 600/700 total=33298.3398 recon=33270.3633 kl=27.9773 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch26\n",
      "[train] epoch 26 step 0/700 total=33296.6680 recon=33271.0703 kl=25.5962 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 26 step 100/700 total=33299.7344 recon=33272.5625 kl=27.1732 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 26 step 200/700 total=33313.6562 recon=33286.2773 kl=27.3782 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 26 step 300/700 total=33225.5039 recon=33198.2031 kl=27.3002 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 26 step 400/700 total=33280.4102 recon=33250.7969 kl=29.6145 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 26 step 500/700 total=33285.3789 recon=33257.0625 kl=28.3171 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 26 step 600/700 total=33240.3594 recon=33213.1289 kl=27.2312 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch27\n",
      "[train] epoch 27 step 0/700 total=33240.1367 recon=33212.7109 kl=27.4251 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 27 step 100/700 total=33238.8242 recon=33210.8125 kl=28.0136 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 27 step 200/700 total=33311.7305 recon=33283.3594 kl=28.3724 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 27 step 300/700 total=33229.3086 recon=33201.0039 kl=28.3032 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 27 step 400/700 total=33273.9727 recon=33245.3672 kl=28.6069 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 27 step 500/700 total=33297.0859 recon=33269.3750 kl=27.7093 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 27 step 600/700 total=33271.0664 recon=33244.5273 kl=26.5373 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch28\n",
      "[train] epoch 28 step 0/700 total=33288.0430 recon=33260.2031 kl=27.8406 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 28 step 100/700 total=33286.1992 recon=33257.4180 kl=28.7822 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 28 step 200/700 total=33299.6914 recon=33269.5898 kl=30.1032 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 28 step 300/700 total=33213.4531 recon=33185.5156 kl=27.9389 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 28 step 400/700 total=33254.3867 recon=33225.8125 kl=28.5747 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 28 step 500/700 total=33332.9727 recon=33304.7734 kl=28.1990 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 28 step 600/700 total=33293.8398 recon=33265.3750 kl=28.4642 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch29\n",
      "[train] epoch 29 step 0/700 total=33241.9102 recon=33214.5547 kl=27.3547 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 29 step 100/700 total=33314.2734 recon=33286.7461 kl=27.5278 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 29 step 200/700 total=33283.8398 recon=33255.7500 kl=28.0892 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 29 step 300/700 total=33248.9141 recon=33221.7773 kl=27.1355 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 29 step 400/700 total=33337.8984 recon=33309.8164 kl=28.0816 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 29 step 500/700 total=33263.4648 recon=33235.3125 kl=28.1528 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 29 step 600/700 total=33224.4648 recon=33195.8984 kl=28.5651 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch30\n",
      "[train] epoch 30 step 0/700 total=33261.9023 recon=33233.3281 kl=28.5747 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 30 step 100/700 total=33248.8086 recon=33220.1445 kl=28.6622 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 30 step 200/700 total=33220.7227 recon=33193.7734 kl=26.9476 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 30 step 300/700 total=33281.8750 recon=33254.4180 kl=27.4590 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 30 step 400/700 total=33243.1836 recon=33214.8516 kl=28.3337 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 30 step 500/700 total=33280.0430 recon=33252.8086 kl=27.2359 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 30 step 600/700 total=33238.8672 recon=33211.4336 kl=27.4323 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch31\n",
      "[train] epoch 31 step 0/700 total=33264.7266 recon=33235.7930 kl=28.9354 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 31 step 100/700 total=33225.2852 recon=33195.9961 kl=29.2890 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 31 step 200/700 total=33270.5508 recon=33243.2578 kl=27.2946 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 31 step 300/700 total=33261.1836 recon=33233.3711 kl=27.8134 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 31 step 400/700 total=33290.2109 recon=33261.4453 kl=28.7675 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 31 step 500/700 total=33260.7617 recon=33232.6484 kl=28.1119 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 31 step 600/700 total=33236.4922 recon=33208.0234 kl=28.4673 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch32\n",
      "[train] epoch 32 step 0/700 total=33263.7031 recon=33236.2578 kl=27.4444 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 32 step 100/700 total=33241.2734 recon=33211.3125 kl=29.9614 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 32 step 200/700 total=33271.9844 recon=33244.0352 kl=27.9490 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 32 step 300/700 total=33270.3438 recon=33241.4961 kl=28.8495 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 32 step 400/700 total=33253.2969 recon=33224.5508 kl=28.7463 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 32 step 500/700 total=33275.9062 recon=33247.1641 kl=28.7441 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 32 step 600/700 total=33217.8477 recon=33189.7461 kl=28.1009 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch33\n",
      "[train] epoch 33 step 0/700 total=33226.9375 recon=33197.1484 kl=29.7881 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 33 step 100/700 total=33244.1719 recon=33215.4297 kl=28.7413 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 33 step 200/700 total=33279.9961 recon=33250.5508 kl=29.4458 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 33 step 300/700 total=33326.6719 recon=33299.0781 kl=27.5939 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 33 step 400/700 total=33206.0195 recon=33177.1953 kl=28.8251 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 33 step 500/700 total=33254.0078 recon=33224.6094 kl=29.3980 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 33 step 600/700 total=33242.3555 recon=33214.1953 kl=28.1596 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch34\n",
      "[train] epoch 34 step 0/700 total=33253.3750 recon=33224.5000 kl=28.8761 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 34 step 100/700 total=33274.1992 recon=33245.4609 kl=28.7396 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 34 step 200/700 total=33244.6914 recon=33215.4102 kl=29.2827 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 34 step 300/700 total=33203.7461 recon=33174.8828 kl=28.8640 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 34 step 400/700 total=33242.6484 recon=33214.4883 kl=28.1585 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 34 step 500/700 total=33186.0000 recon=33156.9727 kl=29.0256 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 34 step 600/700 total=33233.1445 recon=33205.2500 kl=27.8939 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch35\n",
      "[train] epoch 35 step 0/700 total=33215.0625 recon=33185.4648 kl=29.5987 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 35 step 100/700 total=33231.5703 recon=33204.2695 kl=27.3004 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 35 step 200/700 total=33274.2812 recon=33245.7695 kl=28.5133 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 35 step 300/700 total=33229.7383 recon=33201.1445 kl=28.5954 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 35 step 400/700 total=33295.5859 recon=33267.6953 kl=27.8914 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 35 step 500/700 total=33191.6250 recon=33164.8945 kl=26.7301 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 35 step 600/700 total=33238.2188 recon=33210.1797 kl=28.0382 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch36\n",
      "[train] epoch 36 step 0/700 total=33202.4062 recon=33173.1484 kl=29.2565 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 36 step 100/700 total=33281.6953 recon=33253.3594 kl=28.3368 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 36 step 200/700 total=33293.2188 recon=33263.8398 kl=29.3805 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 36 step 300/700 total=33270.8242 recon=33241.3359 kl=29.4871 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 36 step 400/700 total=33251.4844 recon=33223.4531 kl=28.0324 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 36 step 500/700 total=33258.8242 recon=33230.8242 kl=27.9988 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 36 step 600/700 total=33271.7656 recon=33244.7227 kl=27.0442 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch37\n",
      "[train] epoch 37 step 0/700 total=33231.5586 recon=33203.5352 kl=28.0222 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 37 step 100/700 total=33276.3672 recon=33247.0977 kl=29.2693 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 37 step 200/700 total=33241.3164 recon=33210.9492 kl=30.3688 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 37 step 300/700 total=33258.1367 recon=33228.7461 kl=29.3919 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 37 step 400/700 total=33293.0234 recon=33263.5352 kl=29.4875 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 37 step 500/700 total=33245.2461 recon=33216.7734 kl=28.4721 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 37 step 600/700 total=33249.8555 recon=33220.9961 kl=28.8583 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch38\n",
      "[train] epoch 38 step 0/700 total=33314.0117 recon=33284.5352 kl=29.4780 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 38 step 100/700 total=33241.4258 recon=33211.4883 kl=29.9390 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 38 step 200/700 total=33262.4453 recon=33234.0312 kl=28.4148 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 38 step 300/700 total=33266.6562 recon=33237.0156 kl=29.6388 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 38 step 400/700 total=33166.2227 recon=33137.2266 kl=28.9947 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 38 step 500/700 total=33312.2617 recon=33285.3906 kl=26.8723 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 38 step 600/700 total=33255.5195 recon=33227.0703 kl=28.4506 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch39\n",
      "[train] epoch 39 step 0/700 total=33315.3281 recon=33286.6836 kl=28.6463 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 39 step 100/700 total=33240.6914 recon=33212.8281 kl=27.8628 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 39 step 200/700 total=33265.1992 recon=33237.3242 kl=27.8751 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 39 step 300/700 total=33203.0625 recon=33173.8047 kl=29.2573 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 39 step 400/700 total=33265.5820 recon=33237.3555 kl=28.2275 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 39 step 500/700 total=33240.8828 recon=33212.7383 kl=28.1428 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 39 step 600/700 total=33290.6016 recon=33261.5781 kl=29.0217 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch40\n",
      "[train] epoch 40 step 0/700 total=33262.3320 recon=33233.6992 kl=28.6315 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 40 step 100/700 total=33303.8555 recon=33275.8438 kl=28.0133 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 40 step 200/700 total=33274.4141 recon=33244.7930 kl=29.6206 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 40 step 300/700 total=33265.4570 recon=33236.1680 kl=29.2890 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 40 step 400/700 total=33304.5273 recon=33275.5977 kl=28.9305 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 40 step 500/700 total=33320.3086 recon=33293.0273 kl=27.2815 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 40 step 600/700 total=33295.0781 recon=33266.8594 kl=28.2194 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch41\n",
      "[train] epoch 41 step 0/700 total=33254.7500 recon=33225.4688 kl=29.2813 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 41 step 100/700 total=33281.6289 recon=33252.2656 kl=29.3652 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 41 step 200/700 total=33296.0430 recon=33266.2266 kl=29.8159 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 41 step 300/700 total=33257.5938 recon=33229.2500 kl=28.3444 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 41 step 400/700 total=33219.8867 recon=33190.2227 kl=29.6645 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 41 step 500/700 total=33267.3672 recon=33236.8633 kl=30.5054 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 41 step 600/700 total=33295.5859 recon=33265.8203 kl=29.7667 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch42\n",
      "[train] epoch 42 step 0/700 total=33287.8750 recon=33258.5703 kl=29.3039 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 42 step 100/700 total=33289.7266 recon=33261.2227 kl=28.5039 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 42 step 200/700 total=33243.4453 recon=33214.2734 kl=29.1735 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 42 step 300/700 total=33254.7617 recon=33225.2773 kl=29.4863 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 42 step 400/700 total=33276.3633 recon=33246.3281 kl=30.0345 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 42 step 500/700 total=33237.1250 recon=33209.9336 kl=27.1915 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 42 step 600/700 total=33251.0195 recon=33222.0117 kl=29.0065 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch43\n",
      "[train] epoch 43 step 0/700 total=33239.3555 recon=33210.6094 kl=28.7469 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 43 step 100/700 total=33266.4453 recon=33235.6680 kl=30.7772 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 43 step 200/700 total=33244.4727 recon=33215.6875 kl=28.7842 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 43 step 300/700 total=33251.4492 recon=33222.0156 kl=29.4339 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 43 step 400/700 total=33316.1094 recon=33287.7148 kl=28.3959 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 43 step 500/700 total=33288.7422 recon=33260.5977 kl=28.1451 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 43 step 600/700 total=33298.4922 recon=33270.9141 kl=27.5773 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch44\n",
      "[train] epoch 44 step 0/700 total=33279.0508 recon=33250.5195 kl=28.5293 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 44 step 100/700 total=33257.5195 recon=33228.3398 kl=29.1812 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 44 step 200/700 total=33247.3750 recon=33218.3086 kl=29.0673 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 44 step 300/700 total=33184.1680 recon=33153.3008 kl=30.8668 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 44 step 400/700 total=33273.1602 recon=33242.7500 kl=30.4104 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 44 step 500/700 total=33193.5469 recon=33163.9727 kl=29.5757 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 44 step 600/700 total=33278.6953 recon=33251.1484 kl=27.5477 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch45\n",
      "[train] epoch 45 step 0/700 total=33322.9688 recon=33293.4414 kl=29.5275 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 45 step 100/700 total=33222.1953 recon=33190.4805 kl=31.7147 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 45 step 200/700 total=33245.0312 recon=33214.5820 kl=30.4505 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 45 step 300/700 total=33251.3945 recon=33222.2266 kl=29.1694 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 45 step 400/700 total=33263.2461 recon=33235.7578 kl=27.4883 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 45 step 500/700 total=33236.5234 recon=33208.0156 kl=28.5072 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 45 step 600/700 total=33233.7422 recon=33203.5547 kl=30.1866 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch46\n",
      "[train] epoch 46 step 0/700 total=33275.2695 recon=33245.9844 kl=29.2854 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 46 step 100/700 total=33223.2930 recon=33194.5000 kl=28.7948 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 46 step 200/700 total=33246.4414 recon=33213.8828 kl=32.5570 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 46 step 300/700 total=33245.1445 recon=33216.6602 kl=28.4835 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 46 step 400/700 total=33234.3906 recon=33205.3711 kl=29.0179 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 46 step 500/700 total=33156.7539 recon=33124.9102 kl=31.8425 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 46 step 600/700 total=33280.6289 recon=33249.3242 kl=31.3065 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch47\n",
      "[train] epoch 47 step 0/700 total=33292.0781 recon=33261.6484 kl=30.4310 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 47 step 100/700 total=33182.0117 recon=33150.5781 kl=31.4346 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 47 step 200/700 total=33244.5039 recon=33215.8633 kl=28.6388 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 47 step 300/700 total=33241.9805 recon=33213.7227 kl=28.2576 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 47 step 400/700 total=33216.9336 recon=33186.9492 kl=29.9835 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 47 step 500/700 total=33274.6719 recon=33245.4336 kl=29.2370 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 47 step 600/700 total=33219.9297 recon=33189.4023 kl=30.5268 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch48\n",
      "[train] epoch 48 step 0/700 total=33199.9766 recon=33170.6836 kl=29.2925 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 48 step 100/700 total=33199.7891 recon=33170.4531 kl=29.3357 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 48 step 200/700 total=33179.3359 recon=33149.7578 kl=29.5789 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 48 step 300/700 total=33243.4961 recon=33213.1836 kl=30.3129 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 48 step 400/700 total=33234.0820 recon=33205.0859 kl=28.9958 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 48 step 500/700 total=33248.3438 recon=33217.5234 kl=30.8199 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 48 step 600/700 total=33198.7031 recon=33170.4609 kl=28.2436 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch49\n",
      "[train] epoch 49 step 0/700 total=33232.9102 recon=33204.7383 kl=28.1714 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 49 step 100/700 total=33295.4414 recon=33264.8789 kl=30.5637 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 49 step 200/700 total=33254.4141 recon=33225.1094 kl=29.3043 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 49 step 300/700 total=33231.8633 recon=33202.5859 kl=29.2788 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 49 step 400/700 total=33263.3008 recon=33234.8633 kl=28.4386 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 49 step 500/700 total=33191.7227 recon=33161.0703 kl=30.6514 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 49 step 600/700 total=33231.6719 recon=33202.8906 kl=28.7815 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch50\n",
      "[train] epoch 50 step 0/700 total=33296.0781 recon=33266.1484 kl=29.9298 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 50 step 100/700 total=33213.8242 recon=33185.4023 kl=28.4211 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 50 step 200/700 total=33259.4531 recon=33229.0078 kl=30.4453 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 50 step 300/700 total=33269.4961 recon=33240.9258 kl=28.5722 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 50 step 400/700 total=33229.4297 recon=33199.2227 kl=30.2083 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 50 step 500/700 total=33204.5938 recon=33173.6523 kl=30.9404 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 50 step 600/700 total=33257.0156 recon=33227.6133 kl=29.4011 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch51\n",
      "[train] epoch 51 step 0/700 total=33195.1875 recon=33166.6406 kl=28.5472 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 51 step 100/700 total=33227.6758 recon=33198.1445 kl=29.5315 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 51 step 200/700 total=33237.4883 recon=33207.1641 kl=30.3232 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 51 step 300/700 total=33232.2305 recon=33201.4336 kl=30.7968 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 51 step 400/700 total=33212.0078 recon=33183.4258 kl=28.5821 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 51 step 500/700 total=33235.4062 recon=33203.8008 kl=31.6073 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 51 step 600/700 total=33227.8594 recon=33196.8281 kl=31.0316 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch52\n",
      "[train] epoch 52 step 0/700 total=33224.8047 recon=33192.8086 kl=31.9957 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 52 step 100/700 total=33280.3242 recon=33250.3984 kl=29.9257 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 52 step 200/700 total=33189.2266 recon=33158.8086 kl=30.4165 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 52 step 300/700 total=33232.1602 recon=33203.5586 kl=28.6017 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 52 step 400/700 total=33232.2578 recon=33203.2227 kl=29.0351 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 52 step 500/700 total=33231.9961 recon=33201.0859 kl=30.9089 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 52 step 600/700 total=33214.3867 recon=33186.0703 kl=28.3156 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch53\n",
      "[train] epoch 53 step 0/700 total=33194.0078 recon=33165.6758 kl=28.3311 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 53 step 100/700 total=33308.8477 recon=33278.5586 kl=30.2897 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 53 step 200/700 total=33245.3008 recon=33212.9648 kl=32.3347 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 53 step 300/700 total=33270.7461 recon=33239.9844 kl=30.7623 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 53 step 400/700 total=33253.3398 recon=33223.3555 kl=29.9847 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 53 step 500/700 total=33212.5156 recon=33183.4023 kl=29.1143 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 53 step 600/700 total=33300.3281 recon=33270.5195 kl=29.8096 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch54\n",
      "[train] epoch 54 step 0/700 total=33175.9727 recon=33146.6680 kl=29.3036 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 54 step 100/700 total=33247.3750 recon=33217.3359 kl=30.0398 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 54 step 200/700 total=33228.3008 recon=33197.6484 kl=30.6537 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 54 step 300/700 total=33255.0977 recon=33224.7109 kl=30.3867 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 54 step 400/700 total=33278.8750 recon=33249.5625 kl=29.3133 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 54 step 500/700 total=33219.0391 recon=33188.1094 kl=30.9298 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 54 step 600/700 total=33242.2812 recon=33212.9727 kl=29.3080 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch55\n",
      "[train] epoch 55 step 0/700 total=33250.3438 recon=33221.0625 kl=29.2816 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 55 step 100/700 total=33234.1211 recon=33204.8438 kl=29.2791 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 55 step 200/700 total=33147.5156 recon=33117.0469 kl=30.4672 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 55 step 300/700 total=33249.1875 recon=33218.9531 kl=30.2345 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 55 step 400/700 total=33223.4492 recon=33193.0664 kl=30.3820 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 55 step 500/700 total=33175.1758 recon=33145.6055 kl=29.5716 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 55 step 600/700 total=33234.2422 recon=33205.1680 kl=29.0742 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch56\n",
      "[train] epoch 56 step 0/700 total=33227.6289 recon=33198.9531 kl=28.6770 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 56 step 100/700 total=33226.2031 recon=33195.7109 kl=30.4919 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 56 step 200/700 total=33222.7461 recon=33191.9531 kl=30.7916 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 56 step 300/700 total=33179.7070 recon=33149.8086 kl=29.8973 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 56 step 400/700 total=33257.6484 recon=33228.4805 kl=29.1675 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 56 step 500/700 total=33204.7422 recon=33173.5352 kl=31.2077 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 56 step 600/700 total=33230.4766 recon=33200.5234 kl=29.9543 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch57\n",
      "[train] epoch 57 step 0/700 total=33210.0195 recon=33179.5039 kl=30.5155 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 57 step 100/700 total=33288.4492 recon=33257.2852 kl=31.1631 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 57 step 200/700 total=33261.9375 recon=33231.3750 kl=30.5611 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 57 step 300/700 total=33184.2422 recon=33154.1758 kl=30.0681 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 57 step 400/700 total=33228.9414 recon=33197.2031 kl=31.7380 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 57 step 500/700 total=33223.9414 recon=33193.1953 kl=30.7472 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 57 step 600/700 total=33244.7852 recon=33213.0234 kl=31.7617 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch58\n",
      "[train] epoch 58 step 0/700 total=33260.4375 recon=33230.2852 kl=30.1510 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 58 step 100/700 total=33260.5977 recon=33230.8125 kl=29.7843 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 58 step 200/700 total=33226.4648 recon=33195.9180 kl=30.5459 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 58 step 300/700 total=33243.6211 recon=33212.7539 kl=30.8683 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 58 step 400/700 total=33241.2148 recon=33211.9492 kl=29.2666 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 58 step 500/700 total=33193.1914 recon=33162.3008 kl=30.8918 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 58 step 600/700 total=33203.4492 recon=33172.1445 kl=31.3030 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch59\n",
      "[train] epoch 59 step 0/700 total=33201.4805 recon=33170.5000 kl=30.9821 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 59 step 100/700 total=33223.0664 recon=33192.0703 kl=30.9954 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 59 step 200/700 total=33203.2734 recon=33173.3945 kl=29.8797 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 59 step 300/700 total=33261.2695 recon=33230.9102 kl=30.3587 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 59 step 400/700 total=33253.6953 recon=33223.0625 kl=30.6332 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 59 step 500/700 total=33227.9102 recon=33195.8789 kl=32.0306 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 59 step 600/700 total=33251.5703 recon=33220.4805 kl=31.0886 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch60\n",
      "[train] epoch 60 step 0/700 total=33192.6094 recon=33163.0547 kl=29.5551 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 60 step 100/700 total=33222.6758 recon=33192.5078 kl=30.1677 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 60 step 200/700 total=33188.2227 recon=33157.0781 kl=31.1462 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 60 step 300/700 total=33273.1211 recon=33243.1797 kl=29.9413 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 60 step 400/700 total=33210.8633 recon=33180.4609 kl=30.4010 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 60 step 500/700 total=33237.3555 recon=33207.5703 kl=29.7845 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 60 step 600/700 total=33195.1016 recon=33164.2305 kl=30.8702 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch61\n",
      "[train] epoch 61 step 0/700 total=33200.6992 recon=33169.3984 kl=31.3022 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 61 step 100/700 total=33224.4414 recon=33192.9180 kl=31.5219 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 61 step 200/700 total=33249.5977 recon=33218.2812 kl=31.3150 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 61 step 300/700 total=33260.2852 recon=33230.7188 kl=29.5684 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 61 step 400/700 total=33217.8750 recon=33187.1094 kl=30.7668 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 61 step 500/700 total=33204.4883 recon=33173.7031 kl=30.7833 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 61 step 600/700 total=33274.0273 recon=33244.1406 kl=29.8867 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch62\n",
      "[train] epoch 62 step 0/700 total=33210.0938 recon=33180.0000 kl=30.0946 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 62 step 100/700 total=33250.2773 recon=33218.7930 kl=31.4858 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 62 step 200/700 total=33203.8789 recon=33172.6211 kl=31.2568 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 62 step 300/700 total=33220.1797 recon=33187.5820 kl=32.5979 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 62 step 400/700 total=33180.1797 recon=33148.8945 kl=31.2852 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 62 step 500/700 total=33258.6367 recon=33227.2578 kl=31.3771 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 62 step 600/700 total=33199.0898 recon=33167.7461 kl=31.3435 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch63\n",
      "[train] epoch 63 step 0/700 total=33221.2383 recon=33190.0234 kl=31.2152 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 63 step 100/700 total=33276.8789 recon=33246.0234 kl=30.8549 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 63 step 200/700 total=33238.3359 recon=33207.9805 kl=30.3540 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 63 step 300/700 total=33226.5273 recon=33195.2969 kl=31.2292 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 63 step 400/700 total=33237.0469 recon=33206.3242 kl=30.7235 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 63 step 500/700 total=33200.7734 recon=33168.8477 kl=31.9253 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 63 step 600/700 total=33229.3789 recon=33197.5273 kl=31.8520 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch64\n",
      "[train] epoch 64 step 0/700 total=33170.4922 recon=33140.9141 kl=29.5793 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 64 step 100/700 total=33196.3125 recon=33165.1250 kl=31.1879 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 64 step 200/700 total=33240.5547 recon=33208.8086 kl=31.7472 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 64 step 300/700 total=33235.0703 recon=33203.9766 kl=31.0919 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 64 step 400/700 total=33130.1953 recon=33098.3008 kl=31.8947 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 64 step 500/700 total=33243.4336 recon=33213.4336 kl=30.0015 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 64 step 600/700 total=33247.9883 recon=33216.8125 kl=31.1768 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch65\n",
      "[train] epoch 65 step 0/700 total=33165.8906 recon=33133.4922 kl=32.3993 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 65 step 100/700 total=33185.5391 recon=33154.1328 kl=31.4051 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 65 step 200/700 total=33199.8359 recon=33169.7891 kl=30.0482 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 65 step 300/700 total=33253.6172 recon=33222.5234 kl=31.0925 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 65 step 400/700 total=33226.6250 recon=33196.6328 kl=29.9932 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 65 step 500/700 total=33282.3359 recon=33251.5273 kl=30.8082 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 65 step 600/700 total=33205.0898 recon=33173.4492 kl=31.6388 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch66\n",
      "[train] epoch 66 step 0/700 total=33240.2031 recon=33210.2031 kl=30.0007 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 66 step 100/700 total=33215.9258 recon=33185.9570 kl=29.9688 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 66 step 200/700 total=33210.1055 recon=33179.5781 kl=30.5258 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 66 step 300/700 total=33190.0078 recon=33157.0664 kl=32.9421 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 66 step 400/700 total=33223.5312 recon=33192.1289 kl=31.4029 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 66 step 500/700 total=33237.9023 recon=33205.7500 kl=32.1513 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 66 step 600/700 total=33220.1484 recon=33189.3281 kl=30.8191 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch67\n",
      "[train] epoch 67 step 0/700 total=33246.6953 recon=33214.9609 kl=31.7360 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 67 step 100/700 total=33269.2500 recon=33237.7422 kl=31.5071 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 67 step 200/700 total=33172.8672 recon=33141.6445 kl=31.2215 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 67 step 300/700 total=33232.8477 recon=33199.4180 kl=33.4298 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 67 step 400/700 total=33139.5078 recon=33108.1562 kl=31.3510 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 67 step 500/700 total=33237.5781 recon=33205.1836 kl=32.3945 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 67 step 600/700 total=33210.5664 recon=33177.8516 kl=32.7161 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch68\n",
      "[train] epoch 68 step 0/700 total=33186.5625 recon=33154.1055 kl=32.4567 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 68 step 100/700 total=33170.2969 recon=33138.4258 kl=31.8712 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 68 step 200/700 total=33169.4102 recon=33137.0742 kl=32.3345 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 68 step 300/700 total=33214.1680 recon=33182.7070 kl=31.4614 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 68 step 400/700 total=33176.3711 recon=33146.1641 kl=30.2071 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 68 step 500/700 total=33253.0742 recon=33222.1836 kl=30.8909 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 68 step 600/700 total=33192.1250 recon=33160.0469 kl=32.0789 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch69\n",
      "[train] epoch 69 step 0/700 total=33242.9688 recon=33211.7305 kl=31.2366 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 69 step 100/700 total=33276.3086 recon=33244.1445 kl=32.1639 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 69 step 200/700 total=33226.3203 recon=33194.7734 kl=31.5468 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 69 step 300/700 total=33231.9844 recon=33198.9609 kl=33.0236 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 69 step 400/700 total=33179.4258 recon=33149.5078 kl=29.9182 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 69 step 500/700 total=33154.6836 recon=33123.2461 kl=31.4385 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 69 step 600/700 total=33226.7109 recon=33194.9023 kl=31.8100 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch70\n",
      "[train] epoch 70 step 0/700 total=33233.3438 recon=33202.2852 kl=31.0569 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 70 step 100/700 total=33172.7500 recon=33139.1523 kl=33.5967 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 70 step 200/700 total=33215.8477 recon=33183.8086 kl=32.0404 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 70 step 300/700 total=33209.5859 recon=33177.5352 kl=32.0502 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 70 step 400/700 total=33268.8828 recon=33237.3633 kl=31.5214 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 70 step 500/700 total=33184.5156 recon=33154.3906 kl=30.1264 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 70 step 600/700 total=33246.4922 recon=33214.1133 kl=32.3802 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch71\n",
      "[train] epoch 71 step 0/700 total=33266.4766 recon=33237.9258 kl=28.5512 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 71 step 100/700 total=33227.2617 recon=33195.1523 kl=32.1090 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 71 step 200/700 total=33271.7461 recon=33239.3008 kl=32.4445 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 71 step 300/700 total=33230.3516 recon=33199.3555 kl=30.9950 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 71 step 400/700 total=33239.0703 recon=33207.0312 kl=32.0405 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 71 step 500/700 total=33248.6211 recon=33217.0547 kl=31.5670 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 71 step 600/700 total=33227.6992 recon=33197.2852 kl=30.4154 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch72\n",
      "[train] epoch 72 step 0/700 total=33191.7188 recon=33160.2656 kl=31.4523 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 72 step 100/700 total=33216.3359 recon=33184.9492 kl=31.3853 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 72 step 200/700 total=33207.4180 recon=33174.1484 kl=33.2677 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 72 step 300/700 total=33216.6758 recon=33184.1875 kl=32.4893 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 72 step 400/700 total=33270.6016 recon=33238.3242 kl=32.2789 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 72 step 500/700 total=33199.4453 recon=33167.7656 kl=31.6790 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 72 step 600/700 total=33149.2305 recon=33116.4492 kl=32.7794 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch73\n",
      "[train] epoch 73 step 0/700 total=33173.0469 recon=33140.7539 kl=32.2924 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 73 step 100/700 total=33238.8164 recon=33207.7930 kl=31.0220 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 73 step 200/700 total=33252.5430 recon=33221.7305 kl=30.8134 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 73 step 300/700 total=33226.9609 recon=33194.8203 kl=32.1397 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 73 step 400/700 total=33220.0508 recon=33189.0391 kl=31.0127 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 73 step 500/700 total=33204.5078 recon=33173.2500 kl=31.2578 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 73 step 600/700 total=33207.7109 recon=33174.4609 kl=33.2487 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch74\n",
      "[train] epoch 74 step 0/700 total=33250.0000 recon=33218.3359 kl=31.6641 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 74 step 100/700 total=33186.0977 recon=33153.1445 kl=32.9518 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 74 step 200/700 total=33225.6680 recon=33194.1680 kl=31.4995 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 74 step 300/700 total=33166.4727 recon=33134.2734 kl=32.2001 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 74 step 400/700 total=33194.9844 recon=33162.8945 kl=32.0902 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 74 step 500/700 total=33247.3203 recon=33216.1289 kl=31.1919 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 74 step 600/700 total=33233.2891 recon=33201.4961 kl=31.7942 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch75\n",
      "[train] epoch 75 step 0/700 total=33194.4102 recon=33161.6992 kl=32.7128 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 75 step 100/700 total=33235.1367 recon=33203.7773 kl=31.3608 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 75 step 200/700 total=33214.1758 recon=33183.4023 kl=30.7753 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 75 step 300/700 total=33213.1914 recon=33180.8750 kl=32.3161 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 75 step 400/700 total=33255.4102 recon=33223.2852 kl=32.1252 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 75 step 500/700 total=33202.2188 recon=33168.7734 kl=33.4446 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 75 step 600/700 total=33228.6992 recon=33197.4648 kl=31.2341 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch76\n",
      "[train] epoch 76 step 0/700 total=33187.0977 recon=33156.0625 kl=31.0348 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 76 step 100/700 total=33256.0977 recon=33224.8867 kl=31.2098 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 76 step 200/700 total=33170.8789 recon=33136.5742 kl=34.3055 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 76 step 300/700 total=33184.2070 recon=33153.5430 kl=30.6654 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 76 step 400/700 total=33234.9531 recon=33203.0547 kl=31.8981 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 76 step 500/700 total=33274.9961 recon=33243.5625 kl=31.4341 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 76 step 600/700 total=33202.1602 recon=33171.4453 kl=30.7141 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch77\n",
      "[train] epoch 77 step 0/700 total=33209.2109 recon=33176.8125 kl=32.3976 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 77 step 100/700 total=33235.4922 recon=33203.5195 kl=31.9719 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 77 step 200/700 total=33238.3789 recon=33205.1289 kl=33.2485 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 77 step 300/700 total=33228.9180 recon=33198.1211 kl=30.7955 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 77 step 400/700 total=33239.0469 recon=33207.4609 kl=31.5871 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 77 step 500/700 total=33202.6602 recon=33172.0039 kl=30.6553 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 77 step 600/700 total=33188.8516 recon=33156.8477 kl=32.0023 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch78\n",
      "[train] epoch 78 step 0/700 total=33190.9961 recon=33160.5273 kl=30.4682 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 78 step 100/700 total=33244.8359 recon=33214.1406 kl=30.6960 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 78 step 200/700 total=33189.6836 recon=33157.9492 kl=31.7344 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 78 step 300/700 total=33202.7969 recon=33170.7773 kl=32.0211 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 78 step 400/700 total=33227.1445 recon=33194.2969 kl=32.8458 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 78 step 500/700 total=33209.3320 recon=33175.9648 kl=33.3681 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 78 step 600/700 total=33223.5977 recon=33191.7344 kl=31.8619 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch79\n",
      "[train] epoch 79 step 0/700 total=33182.6250 recon=33150.0742 kl=32.5522 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 79 step 100/700 total=33202.4062 recon=33170.7734 kl=31.6345 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 79 step 200/700 total=33208.4023 recon=33176.1953 kl=32.2065 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 79 step 300/700 total=33279.9688 recon=33247.9648 kl=32.0049 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 79 step 400/700 total=33209.9023 recon=33176.8750 kl=33.0263 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 79 step 500/700 total=33160.8828 recon=33128.1328 kl=32.7510 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 79 step 600/700 total=33207.9023 recon=33175.5781 kl=32.3238 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch80\n",
      "[train] epoch 80 step 0/700 total=33206.2070 recon=33174.1133 kl=32.0943 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 80 step 100/700 total=33209.0391 recon=33177.9727 kl=31.0649 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 80 step 200/700 total=33208.6758 recon=33175.8711 kl=32.8030 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 80 step 300/700 total=33146.4219 recon=33113.8555 kl=32.5664 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 80 step 400/700 total=33180.0664 recon=33147.9883 kl=32.0763 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 80 step 500/700 total=33228.2891 recon=33196.4141 kl=31.8751 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 80 step 600/700 total=33232.0195 recon=33200.2930 kl=31.7271 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch81\n",
      "[train] epoch 81 step 0/700 total=33221.0859 recon=33188.3086 kl=32.7782 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 81 step 100/700 total=33191.3125 recon=33158.7422 kl=32.5686 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 81 step 200/700 total=33165.1250 recon=33130.9023 kl=34.2229 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 81 step 300/700 total=33200.0078 recon=33167.7344 kl=32.2749 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 81 step 400/700 total=33237.4531 recon=33202.6094 kl=34.8432 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 81 step 500/700 total=33196.5234 recon=33163.1562 kl=33.3686 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 81 step 600/700 total=33200.3594 recon=33169.4727 kl=30.8868 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch82\n",
      "[train] epoch 82 step 0/700 total=33211.8594 recon=33179.2852 kl=32.5758 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 82 step 100/700 total=33263.6914 recon=33231.1484 kl=32.5430 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 82 step 200/700 total=33212.0781 recon=33181.3711 kl=30.7061 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 82 step 300/700 total=33217.1055 recon=33183.8125 kl=33.2930 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 82 step 400/700 total=33167.3828 recon=33134.9023 kl=32.4799 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 82 step 500/700 total=33217.6211 recon=33184.2734 kl=33.3483 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 82 step 600/700 total=33167.1250 recon=33133.8945 kl=33.2289 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch83\n",
      "[train] epoch 83 step 0/700 total=33240.3359 recon=33206.8398 kl=33.4961 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 83 step 100/700 total=33235.5469 recon=33202.9922 kl=32.5540 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 83 step 200/700 total=33267.8516 recon=33236.7227 kl=31.1308 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 83 step 300/700 total=33130.4453 recon=33097.3750 kl=33.0711 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 83 step 400/700 total=33238.7500 recon=33205.8398 kl=32.9115 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 83 step 500/700 total=33208.8672 recon=33177.3555 kl=31.5107 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 83 step 600/700 total=33214.7344 recon=33181.9258 kl=32.8100 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch84\n",
      "[train] epoch 84 step 0/700 total=33190.2539 recon=33158.4023 kl=31.8519 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 84 step 100/700 total=33171.0977 recon=33137.6719 kl=33.4264 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 84 step 200/700 total=33236.0938 recon=33203.7891 kl=32.3034 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 84 step 300/700 total=33180.8945 recon=33149.8828 kl=31.0132 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 84 step 400/700 total=33221.8047 recon=33190.9336 kl=30.8691 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 84 step 500/700 total=33267.2070 recon=33235.9062 kl=31.2997 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 84 step 600/700 total=33222.7383 recon=33190.6133 kl=32.1250 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch85\n",
      "[train] epoch 85 step 0/700 total=33225.5391 recon=33193.9414 kl=31.5964 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 85 step 100/700 total=33188.0312 recon=33155.1133 kl=32.9171 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 85 step 200/700 total=33205.9219 recon=33173.0547 kl=32.8686 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 85 step 300/700 total=33167.8320 recon=33135.5586 kl=32.2717 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 85 step 400/700 total=33150.6602 recon=33118.3711 kl=32.2910 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 85 step 500/700 total=33257.1914 recon=33225.6133 kl=31.5798 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 85 step 600/700 total=33240.1172 recon=33207.4609 kl=32.6547 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch86\n",
      "[train] epoch 86 step 0/700 total=33198.3711 recon=33165.5312 kl=32.8398 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 86 step 100/700 total=33215.0039 recon=33181.8477 kl=33.1580 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 86 step 200/700 total=33232.5508 recon=33198.7656 kl=33.7850 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 86 step 300/700 total=33165.6680 recon=33133.1484 kl=32.5192 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 86 step 400/700 total=33165.9766 recon=33133.6328 kl=32.3433 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 86 step 500/700 total=33246.0859 recon=33214.1211 kl=31.9633 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 86 step 600/700 total=33204.7188 recon=33172.2500 kl=32.4677 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch87\n",
      "[train] epoch 87 step 0/700 total=33179.8711 recon=33146.3125 kl=33.5588 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 87 step 100/700 total=33183.2656 recon=33149.5000 kl=33.7637 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 87 step 200/700 total=33160.5312 recon=33127.1484 kl=33.3821 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 87 step 300/700 total=33226.8203 recon=33193.0508 kl=33.7702 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 87 step 400/700 total=33162.9961 recon=33130.4844 kl=32.5130 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 87 step 500/700 total=33163.8320 recon=33130.4688 kl=33.3635 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 87 step 600/700 total=33180.7852 recon=33147.4531 kl=33.3303 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch88\n",
      "[train] epoch 88 step 0/700 total=33157.8359 recon=33123.4180 kl=34.4165 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 88 step 100/700 total=33215.6094 recon=33182.8789 kl=32.7313 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 88 step 200/700 total=33206.7109 recon=33174.4648 kl=32.2442 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 88 step 300/700 total=33222.8789 recon=33190.7031 kl=32.1746 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 88 step 400/700 total=33173.4609 recon=33140.1328 kl=33.3295 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 88 step 500/700 total=33318.7344 recon=33286.3359 kl=32.3989 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 88 step 600/700 total=33218.3594 recon=33184.7734 kl=33.5842 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch89\n",
      "[train] epoch 89 step 0/700 total=33211.2227 recon=33178.9062 kl=32.3145 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 89 step 100/700 total=33221.7344 recon=33188.7383 kl=32.9978 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 89 step 200/700 total=33231.2188 recon=33197.5586 kl=33.6585 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 89 step 300/700 total=33204.2070 recon=33172.5273 kl=31.6794 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 89 step 400/700 total=33200.8906 recon=33167.3711 kl=33.5213 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 89 step 500/700 total=33167.2578 recon=33133.9336 kl=33.3223 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 89 step 600/700 total=33200.4453 recon=33167.6953 kl=32.7488 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch90\n",
      "[train] epoch 90 step 0/700 total=33254.9531 recon=33222.2188 kl=32.7341 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 90 step 100/700 total=33147.3086 recon=33113.5586 kl=33.7501 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 90 step 200/700 total=33209.1602 recon=33177.2031 kl=31.9556 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 90 step 300/700 total=33200.0859 recon=33167.8906 kl=32.1942 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 90 step 400/700 total=33227.7227 recon=33194.3008 kl=33.4236 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 90 step 500/700 total=33233.2070 recon=33200.2031 kl=33.0048 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 90 step 600/700 total=33196.1836 recon=33163.7656 kl=32.4168 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch91\n",
      "[train] epoch 91 step 0/700 total=33192.8477 recon=33159.6992 kl=33.1490 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 91 step 100/700 total=33178.6094 recon=33146.1836 kl=32.4268 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 91 step 200/700 total=33118.7109 recon=33084.2305 kl=34.4791 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 91 step 300/700 total=33175.5508 recon=33141.6211 kl=33.9310 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 91 step 400/700 total=33206.8125 recon=33174.3828 kl=32.4303 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 91 step 500/700 total=33203.9062 recon=33170.2930 kl=33.6144 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 91 step 600/700 total=33183.1172 recon=33150.0586 kl=33.0575 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch92\n",
      "[train] epoch 92 step 0/700 total=33230.0195 recon=33196.5781 kl=33.4400 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 92 step 100/700 total=33244.8242 recon=33213.3477 kl=31.4760 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 92 step 200/700 total=33166.2969 recon=33132.7500 kl=33.5480 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 92 step 300/700 total=33245.1055 recon=33211.8086 kl=33.2979 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 92 step 400/700 total=33168.0625 recon=33134.3125 kl=33.7486 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 92 step 500/700 total=33197.3984 recon=33166.2344 kl=31.1659 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 92 step 600/700 total=33207.0312 recon=33175.2734 kl=31.7594 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch93\n",
      "[train] epoch 93 step 0/700 total=33160.0820 recon=33126.8086 kl=33.2723 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 93 step 100/700 total=33221.7266 recon=33188.7227 kl=33.0043 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 93 step 200/700 total=33193.3242 recon=33160.9414 kl=32.3810 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 93 step 300/700 total=33179.8398 recon=33146.1992 kl=33.6389 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 93 step 400/700 total=33159.1406 recon=33125.2461 kl=33.8954 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 93 step 500/700 total=33200.8711 recon=33167.5508 kl=33.3210 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 93 step 600/700 total=33219.3789 recon=33185.3125 kl=34.0650 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch94\n",
      "[train] epoch 94 step 0/700 total=33280.9336 recon=33248.2227 kl=32.7124 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 94 step 100/700 total=33223.7461 recon=33190.2656 kl=33.4805 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 94 step 200/700 total=33234.7148 recon=33202.1055 kl=32.6096 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 94 step 300/700 total=33165.2617 recon=33130.6680 kl=34.5943 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 94 step 400/700 total=33215.5547 recon=33182.5742 kl=32.9798 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 94 step 500/700 total=33183.3945 recon=33151.5781 kl=31.8175 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 94 step 600/700 total=33173.0859 recon=33139.3633 kl=33.7220 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch95\n",
      "[train] epoch 95 step 0/700 total=33237.4023 recon=33205.8945 kl=31.5089 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 95 step 100/700 total=33148.2617 recon=33114.8125 kl=33.4486 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 95 step 200/700 total=33276.3867 recon=33244.0703 kl=32.3157 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 95 step 300/700 total=33184.0781 recon=33151.2109 kl=32.8680 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 95 step 400/700 total=33209.6953 recon=33176.6406 kl=33.0555 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 95 step 500/700 total=33193.6250 recon=33159.6758 kl=33.9506 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 95 step 600/700 total=33266.8164 recon=33233.9688 kl=32.8480 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch96\n",
      "[train] epoch 96 step 0/700 total=33222.2227 recon=33190.5391 kl=31.6819 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 96 step 100/700 total=33238.6133 recon=33205.9727 kl=32.6411 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 96 step 200/700 total=33215.7031 recon=33184.1602 kl=31.5425 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 96 step 300/700 total=33150.7969 recon=33117.2148 kl=33.5820 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 96 step 400/700 total=33231.9844 recon=33197.5938 kl=34.3896 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 96 step 500/700 total=33194.4453 recon=33160.8086 kl=33.6380 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 96 step 600/700 total=33263.2109 recon=33231.0000 kl=32.2116 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch97\n",
      "[train] epoch 97 step 0/700 total=33228.4570 recon=33194.9297 kl=33.5285 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 97 step 100/700 total=33211.5156 recon=33178.1133 kl=33.4024 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 97 step 200/700 total=33215.2070 recon=33182.2695 kl=32.9368 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 97 step 300/700 total=33213.5664 recon=33180.8633 kl=32.7019 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 97 step 400/700 total=33153.3008 recon=33119.4062 kl=33.8945 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 97 step 500/700 total=33184.6875 recon=33150.8945 kl=33.7937 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 97 step 600/700 total=33199.8281 recon=33167.0977 kl=32.7321 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch98\n",
      "[train] epoch 98 step 0/700 total=33275.4414 recon=33243.8438 kl=31.5966 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 98 step 100/700 total=33208.3086 recon=33174.7773 kl=33.5325 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 98 step 200/700 total=33263.4727 recon=33230.6758 kl=32.7958 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 98 step 300/700 total=33178.3867 recon=33145.6133 kl=32.7746 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 98 step 400/700 total=33149.0352 recon=33115.4102 kl=33.6249 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 98 step 500/700 total=33197.3906 recon=33162.9453 kl=34.4438 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 98 step 600/700 total=33188.9648 recon=33155.8086 kl=33.1559 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch99\n",
      "[train] epoch 99 step 0/700 total=33202.9648 recon=33167.9961 kl=34.9689 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 99 step 100/700 total=33248.5625 recon=33215.5586 kl=33.0024 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 99 step 200/700 total=33219.7109 recon=33186.7930 kl=32.9165 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 99 step 300/700 total=33198.2812 recon=33165.1406 kl=33.1407 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 99 step 400/700 total=33173.8359 recon=33140.1250 kl=33.7129 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 99 step 500/700 total=33250.1406 recon=33216.2344 kl=33.9078 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 99 step 600/700 total=33175.2891 recon=33142.1797 kl=33.1112 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "epoch100\n",
      "[train] epoch 100 step 0/700 total=33202.1992 recon=33170.5234 kl=31.6752 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 100 step 100/700 total=33126.4648 recon=33092.1602 kl=34.3059 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 100 step 200/700 total=33197.2305 recon=33163.6875 kl=33.5448 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 100 step 300/700 total=33229.8828 recon=33197.2305 kl=32.6543 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 100 step 400/700 total=33211.7773 recon=33178.3555 kl=33.4226 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 100 step 500/700 total=33224.9844 recon=33193.6602 kl=31.3226 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n",
      "[train] epoch 100 step 600/700 total=33205.0234 recon=33171.7930 kl=33.2301 loss_color=0.0000 loss_shape=0.0000 loss_count=0.0000\n"
     ]
    }
   ],
   "source": [
    "result = fit(revae_128, train_dl_128, optimizer128, cfg128, val_loader=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
